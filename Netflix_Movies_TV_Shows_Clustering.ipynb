{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kaushik0908/Netflix-Movies-And-TV-Shows-Clustering/blob/main/Netflix_Movies_TV_Shows_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised Learning\n",
        "##### **Contribution**    - Individual/Team\n",
        "##### **Team Member 1 -** Kaushik Dey\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![netflix-logo.webp](data:image/webp;base64,UklGRloZAABXRUJQVlA4TE4ZAAAv/8lnEa9AJm2TXvPveSIUtG3DpN3nz2X4BAIpbIMVspEE7ePvdgpncAQ3CQAM8gIiJJVJk8lWurpfcA8LQ2LcRpKi6so/62U4eEbEBFDPTyllImWhSQDVOEgpiQCpF8fSvcZJTKDMuIPMdyaQiWP0qwII0gNWM5tmPjYF2bbjttEHHoAH4EFJT/vfLKW2BnyblMgwov8TwLiNJEXPxfyjpoOWHNH/CfBt224lSZJtAUKCJGiQutf/f2yGu5upTmFYzvDpLaL/E2Cn//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf//mf/53cG+pQUuaKn0hj0pqTxk/8fThWRr3OmFf+PrNQX4/GpDWVQv9cBb/RRdclie56qsBndJG6pKFbT1Xwgy9Klxi6eqiGzwgjdMlAF890w1uMsXWJoduP5AXfGaNcl2xw5U+U8LZRxtAlHd18ooLfOKN0iaGrBxr4jDQuXbLQ9ecp+MEaS5c4unycjs9Yo3SJ3eDqcW54kzeaLgl042G84DtvpC4xdPUwCS+NN0qYJLrrWQp+Y47QJRe6fJSBz5hj6xJDV49S8IM6SpgMdPEggc+4Y+gSQ3c/yA1vkkcJkwRX/hhXwXf2cF3S0K3HSHhp7DF1iaGrxyj4jT5KmEx0/SEmvNv449Iljm4/RMEPAkldYhtc+SMEPiOQEiYd3XyEG96kkK5LDF09wVXwjUJSmCx07QESXnJICRNHlw9Q8C8SCV1i6ArfhHcbidzCJNAFvIIfLFKuSwxdoQt8RiNDmCS6C1zBnzxSwuRCl9gaPiOSS5cYusKW8JJJljAZ6AKZF3xnkhImhu5GNuHdRiVdmCS4cmAFv3NJCpOObuEKfMYlJUwMXeEq+INNQphMdB1Vw2dssoWJo9uoEl7SSbkusRtcgfKC73wyhEmgG5gWvNv4pISJoStMBb8ziguTha4hCnzGKEuYXOgSUcEPSilhYugKUMNnnNKESaALPBveIpUUJoau4HjBd1IpZZLoHM2Ct41VujBp6Baagt9oZQsTQ1dgAp/RSimTgS6wFPwglhAmjm5DafiMWEqY2AZXjuSGt6jFhUlHN4F4wXdqmcLE0BWQhLeNWkqZLHQdR8Fv5HIJE0eXMAY+I5clTOwGVzAKfrBLKZNAN0B0fEYvXZgYugJxw5v8ksok0V0QvOA7v5QyudAlhISXRjAhTAxdQSj4jWG2MhnoAsDEZwxTLkwM3Q2g4AfHDGWS4MrfLvAZx5QyaejW293wJstcwsTQ1btdBd9ZZimTia6/WcJLY5lSJo5uv1nBv3imCxPb4MrfasK7jWdSmXR0460KfhBNKRNDV+8U+IxpQpksdO2NbniTarYycXT5PlfBN6opZWLo6n0SXpLNUCaBLt7FC/5FNqVMDF29y4R3G9u4Mkl015sU/KCbpUwudPkegc/oppSJoav3KPiTcC5lMtDFOzR8RjipTAzd/Q4JLxmnpEmCK389L/hOOf276j9bW3HZkeKyn+mr19HN15vwbqOc/V1lq+sHKv2o9H5m6Or1Cn7nnJImE11/tcBnpBPKxNHtVyv4g3VuZWI3uHqxhs9Yp1yZBLrxWhte8s5QJoauXsoLvvNOSZOFrr3Sgncb8VzK5EKXr1TwG/MsZWLo6oUCnzFPSZNAF69T8IN7ujIxdPUyDZ9xT0qTROevsuEt8ilp0tCtF/GC7+wTysTQ1YsseNvYZ0uTgS5eo+A3+ilXJo5uv0TgM/4ZysQ2uPJXKPhBQCVNOrr5Ah2fMZArE0NXL3DDWxS0pMlC177MC75TUEkTR5dflvDSOKgpE7vB1ZcV/EZCKU0C3fiigc9IqKSJoasvKvhBQyFNEt31JR2f0dCWJhe6/JIb3uShkiaGrr7CC74TUUiTgS6+IOGlEVFJE0N3f0HBb1Tk0iTBlX/axGdUNKVJQ7c+reAHF5U0MXT1WYHPyOiSJhNd/6Qb3mSjlCaObn/OVfCNjUqa2AZX/ikJL/moS5NANz6l4F98lNLE0NVnTHi38VFpk4WufULBD0YKaeLo8tcCnzHSLU0MXf3aDW9SUrk0CXTxK63gGycNaWLo6lcSXpJSaZNEd/2cF/yLlS5pcqFbPzfh3cZKS5oYuvq5gh+0VNpkoIufCXzGS12aGLr7Zwr+IKbUJhtc+Y8aPiOm0iYd3fxRwktqCmli6OoHXvCdmrY2mej6X014t1FTaRNHt/+q4HdyGtLEbnD1F4HPyKm0SaAb/1LwBz25NDF0ZWYNn9HT0iYL3WW24SU/lTa50KV5wXeCatLE0JX9J7xtBJXaJND9Z8FvDFXaxNA9oFFU1yZ5eMFRW5u0wzOOKm1iZ7dYKrTJODpnqdImfnLbaMqlie2Dazw1tEk/OOOp0iZ2bsFUlzZZx2ZMtbSJn9qkqtImdh+ac1XXJnFmaVyV2sTOrJFViZM8MmOr0CbXiQVdbW1iJ2Z0Va5NxnlNwhraxM7LCavESZ5WGmNd2qSdVqOspU3stIyySpzMswrSatrEz8pIK7WJ7ZOarFXiJE7KaCu0iR1U8tYWJ+ucLt4qceLHdBtxDW1ixxTMVeIkTsmoy7WJHdLkrilO8oyMu0qctCNK9rq0iR3RxV4pTsYB3cZeJU7sgDp/dXGyz8f4a4uTfjyDwEqc2PEYg4U4mYeTFHaLEz8cp7BybWL30dzGYUOcxNF0EitxYkdjLHaJkzyYQWNLnFwHYzRW4sTOZRFZFydxLE5kKU7sVLYRWamTPJRGZSFO2qEYlW1xYmcSXFb+z6chr0loHImR2fgHlNcs5Cey2KzEie0DcTpzcdLPYxudre8rPJbPnfZ87Dwan9X3ytA9h4Pc9VnHYYTWvlec3vfx0whGS3Fi92EYo5U6ibOYnNbFiZ2Fc9pWJ3kSaZxW6uQ6icZqIU7sJIzVSp2Mcwhec3Fi52C8NtVJnsIktlIn/RSc2S5xYoeQxmxLncwzaNRW6sSP4DZu6+LE9gkEuaU6iRMwcit1Ygcw6S3UyXo+o7dbnfjjJb+VixN7vIvghjqJh7uN4EqdmHNBcZc6ab4ZxS11Atcmx5U6MdeM5Lo6KY4ly6U6Mcec5UqedLduo7lQJ8mtznNbnZhbxnMlT+jUYLqhToJTxnSlTuziUnKdq5PsknPdUifm0W1cV/KkOdTZ7lIn0SFju1Qn5k9w038gKnmS3TFu+l9IXZ2YN4uc/v9GtOVJc8bZ6X8QlTyBL9vYySCFOjFfGjuZJaJbnhRXjJ86onJ1EjwJgjJIQ51Yd8QYKhGVPEl+LIrqkC51Yn44RRmkJU+qF9s4aiEqeRK8aCR1QerqxC5OGEkZpJQn2YegqYWo5In5YDTlkEKeNA8mTxmkLU+iB05UA1G5OjEH0ojKIQ15UvA1pjJIJU+Iz6hqQLrUyYAfXGWQljjJ81tktRGVOLnMr8gqIDVpEoeDnasMUkqT5sEmq42opMlwkawCUggT+jC4yiBtYTKcJKtEVLoke9G4qkMKWXLxIrnKIJUqwXCTrBKSi5LmR3BVgzQ1SRiOcpVBKk1CTy6uWpAuSTI8XVx1QUpFkl0prjJIpUiGr8FVC1LXI3Bmc5VDSj3SnCmnKoNUciQMbydXDUihRuhOcZVDutXI8LdTlUEq/ycQvaak7FBy1YA0/gEk+5mSxvCYqwxSSRG4NLhqQ7qUSHepuCogre8hPbDykV0394LC8PmiKoNU3wy/+fWEL0dbFbPEeqjkqg2pfy/Y6pol6qm5qkNKGRKPFVRlkEqG1GPfXJWQQoS05yqnqg5pi5D9YIuqDFJpEK8n56qENCTIerROVRekkiD16JuqDJMLkPFs5VS1IC0BUg8/qeqCVPqjP11RlWFq8uN+vEZVE1KqD6/HT6pySKU+8vmKqgxTFx91gEFVA9LWHuMEiqocUmmPOsKLqQxTKI9+BklVAamUx30GRVWGyXWH1yEGVW1IQ3fkKdxUFZDKSnXUMTpTGab/Ux3zHCZVbUj/qTrqIKmqQ8KtDuIkOlOZkLlPYlNVypirjpKquozJsxhMZTKmDpOqUsTM02hMdYmYOs1kKtMwcRxFVUvC3OcRTHUpmFYHylSmYPJEnKmmfvE60cVUrl/mkRRTmX6pMw2mGuolDmUzlauXOlUnKru1SzuWyVShXfJYiqlMunida2eqrVzmwSRThXKpk2UqEy5xNIOpUrfU2TJVly3tcC6iMtmyDyeZKkWL1+kyVRct63iCqEy01PHeTLUkS5xPOVFdkqUOeBGVKZZ2QsVUS7DsI+pEdekVryPeRGV6ZZ1ROVENuVKHPIjK1co4pSIqUyt1zI2ohlbp55RE5VrlPqciKruVitdBB1GFUsmTKqIypVJHfRHV1injrJKoQqfUYROVyZR+WkFUqVLu07qJqouUq47becpESp7XJKrUKHXgRNUlyjyxzlMmUerEN1EtgRJHVkR1CZT7zAZPmT656tCJasmTPLXGU5c8qVNPnjJ1Mo+tiGqIkzr34CnXJnFwxVOmTerknaeGMmlHt3jKlUkeXfGUbV3idfbBU6FL5uFtnjJdUqfvPLVVSeD7b3STp0KVFPz/MnTFUyZKGj6zia7zVGqShJdmji55qksSL/huZje44imTJBPebWYW6AZPpSIp+P1fDF3xVBckgc/+cqG7aMoEScEff3WhS55acqThsx+iK5665MiGlz8KdEFTpka84PuPDN3NU0uMLHi3/WSCK6epS4wU/PYzDd2iKdMigc9+Gl3x1JAiBT9+bqDrNOVKpOGzn3d0m6ZMiWx46xdsgyunqaFDvOD7r3R0g6ZchyS8bb+MrmjKtgwp+O3XFrpGU6FCBj77dUeXNGUqpODHJ9gNrnhqa5COzz4z0AVNhQa54a1PMXRFUyZBvOD75yS6i6byNzb/YDNJeGmfe6FLmuq/sbktSZ1LLLeIgt8+ydAVTdlvcO6rJUkWAIgr38Bnnz3QBU3lPs99JamSLAAQ1rSCH59m6G6aavtB95SkRpIAsF51fPb5Ca6cpWwrAoBCskkaHyP3lCSSTADCWnTDm1/Q0U2aqttMAJBJVknjjh8sd5QkkgSAleYq+P4Fhq5oKm4jAUAm2SSNJ/yk+bokVZIAsJIkvLSvnOg6S9lGEQGQrJLGC37yfF1SJ1kAhJWi4LcvcXSbpupGEAGQrJLG638SfV1cGyY++9oNrmgqrHYAMklJY7qfTGN1KPjxRYFusJStaAAKWSWNye+oBD77anRFU+M7C0Amq6ThalC54c0vW+gaS/n3UgBAskvD8ZxyFXz7ckeXLGXfPgFIJCWN72JOSXj5dYauaGp8y/yTSEoa39KcUvCvFwh0wVL2LfO9jSkT3m2viK5oaouOgh8vkeicpUJzBD57yYZusZRpjoI/X8PQFU1txdHw2YsOdMFSoTgSXr6KodssZYLDC/71MhtcOUul3pjwbnvZjm6yVNcbBb+/jqErljK5EfjshSe6zlKpNgr+eCVHlyzVxEbDZy99gyuWMrGR8PK1At1gqSU1vOD7axm6YqlLakx4t714ortIyqRGwe+vdqFLllpCI/DZy6MrlnKhUfDH6wW6ICnTGQ2fvSG6m6WGzNjw8h0SXDlJucrwgu/v0NAtkjKVseBte0t0xVIhMgp+e4+BrpOUaYzAZ+/p6DZLbYlR8ONNbIMrJ6lQGA2fvWtHN0jKFMYNb72NoSuW2vrCC76/z0LXSKrri4S37X0dXZKU6YuC397IbnDFUqkuBj5750AXJNXVRcGPtzJ0RVImLjo+e+9Ed5FUaosb3nyzC12S1CUtvOD7mxm6IimTFgkv7d0HuiCppSwKfns7Q3eT1CUsBj57/wRXzlEmLAp+AOjoJklNWRH4DCG6IimXFTe8CWGi6xxlquIq+A7B0W2SGqIi4aVh3OCKpFxUFPwGItANjjJNMeHdhhJdkVRIioIfMBa6xlGmKAKfwXR0SVJbUNzwJg5DVyQVeuIq+AY00AVHmZ5IeInE0BVJbTlR8C8oic45qquJCe82qA3d4ihTEwU/sBi6IqnUEoHPwA50wVFdSxT8icbQbY4yKdHwGdwNrpyjUkkkvMTT0U2OuoSEF3zHY+iKo0xITHi3AZ7oOkctHVHwOyJHlxx1yYjAZ5BvcMVRJiMK/sAU6AZHDRHR8BlodMVR/g8OTJuzEl5DleguirJ/cKysW5cX/IDqQpccNYhJNxv58bTgXQw2uuIopx1db7xecNM26q2r4Kd5BbqgKLt5Rdd5PeFqtI0+key6ukUFPps4upujgkpw1d4jI67yatPVzabgl5kluHKKMip5E41YFi67lltHw2czb+gWR+3fjtwfy8Rll6TtYMOrUzN0xVHx25R7RwDIJFm1XOG84Ie5TXSdouw3L3fGkkstV64Fr9vcHd3mqPwNzX0BIJNkl7QqFXxMzja4corqv8m5ZwAAkmyStNoEPpt9Rzcoyv7NIqmR5CfIPQOARJKStJYU/Dw9Q1cclf/WkNRJEgDsAzcASCQpSetEw2fzX+gaRfV/K0hqJDOAYJ/IAFBIdknrwA2vOuDokqLsW09SJ1kARPvEBgCSVdLPzAt+cMBucMVR61tNUieZAdgHegBQmH5QCa+bh4EuKOr65pJEsgCw3cUDKvhwwdAVRdm3k9RJJiDazuP5DHzmY6K7KGp9/0idZAKi7UqeT8HPTlzokqKu7xhJlcwAbMfyeDo+8xJdUZR9k0gimYBge5nHc8OjGwNdUNT4tpBUyQQE2+9srQ2ySd8PL/jBDUN3U5R/K0iNzEC0PVIAJCV9CxJeMz8TXDlD2d89SSQTYLuqEchkk9wq+HCko5sUNf6GSSIJwPZhgURWyZOBzzxFVxTlf5ckkgBs3xbIZJNmV/CzKxNdZyi7/8ZIjUyA7fIGoJBdmlDgM1cd3aao+LshkQWIticMJLJKs7jh0Rfb4Iqi7O+A1MkM2C72VfCDM4FuUNQ+OWbA9rYTXjNv0RVFxcntchf86M5C1xjKtMKEdzF3L3RJUSkVCn72x9AVRXWlEPjM4UAXDGVKwVqbMz8+cNEjQ1cUlUrhJ721OfPjA425nOicobpk+Flvbc78+IDQfGroFkPZE2jP7me9tTnz452iT4auKGrBkioLYDv8V+tzfny83MWcHuiCoS406mRCsGNAoLBJT5K9MnSboQyD1FgAOyyMSKT0GHN7gytnqPVOEpkR7UgRIJvuUfzq6CZDXW8gkQnBjh2BwibdMsfRFUPZ66iTgB1ORmTW5tlC1xlqfJU6CdhxraNLhvJP+hCZEOwA9wZXDGU/9/GxZm9uSjfQDYYaVfWRM9plmhddMZT8TXTXn89c6PLPZwxd/QFNoIs/nzF09x/QJLjyP59p6Nafzxi6+gOaia7/+Yyj238+Yxtc+Z/PWOtzfQAbtPEhtG4GZDZNaLxdqLM0N+UdUSjNBG8TqgTsSByJVVNobw/qzAh2XB5Q2PVS431BlYAdpyOx6kXy24E6C4IdtQcUdj3ZeB9QY4IdxSOx6Wni9icS0Y7pI1j1uLbdqbMg2BF+QKF0v7HBqTPDjviR2XWPvKGpMcGO/5HY9L8uW5gaE+xPA5HYtBhh00pEtD8bRCI2rdP//M//p//5//Q///P/6X/+P/3P//x/+p//T//zP/+f/uf/0//8z/+n//n/9D//8//pf/4//c///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///M///K/B)\n",
        "Netflix, the world’s largest on-demand internet streaming media and online DVD movie rental service provider.it Founded August 29, 1997, in Los Gatos, California by Marc and Reed. It has 69 million members in over 60 countries enjoying more than 100 million hours of TV shows and movies per day Netflix is the world’s leading internet entertainment service with enjoying TV series, documentaries, and feature films across a wide variety of genres and languages. I was curious to analyze the content released in Netflix platform which led me to create these simple, interactive, and exciting visualizations and find similar groups of people.\n",
        "\n",
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Fixable which is a third-party Netflix search engine. In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset. Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Kaushik0908/Netflix-Movies-And-TV-Shows-Clustering"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n",
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\n",
        "\n",
        "In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.\n",
        "# **AIM**\n",
        "Our objective is to conduct an Exploratory Data Analysis to understand what content is available in different countries and if Netflix has been increasingly focusing on TV rather than movies in recent years. And use these insights to cluster similar content by matching text-based features."
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary**\n",
        "This project's goal is to do a cluster analysis on a Netflix dataset in order to identify trends and group items that are similar together. Information on films, actors, directors, genres, and countries was included in the collection.\n",
        "\n",
        "The project took a methodical approach to cluster analysis:\n",
        "\n",
        "\n",
        "1.  **Data Preprocessing:** To manage missing values, eliminate irrelevant columns, and convert category variables into numerical representations, the dataset was cleaned and preprocessed. Text data was also processed by eliminating stopwords and tokenizing them.\n",
        "\n",
        "2.   **Feature Extraction:** Term Frequency-Inverse Document Frequency (TF-IDF) was used to vectorize text data in order to turn textual information into numerical characteristics that could be utilised for clustering.\n",
        "\n",
        "3.  **Dimensionality Reduction:** To minimise the dimensionality of the feature space while keeping significant information, Principal Component Analysis (PCA) was used. This stage aided in successfully visualising and grouping the data.\n",
        "4.   **Clustering Algorithm Selection**: We explored a number of clustering techniques, including K-means and agglomerative clustering. Using assessment measures including the Davies-Bouldin Score, Calinski-Harabasz Score, and Silhouette Score, the total number of clusters was calculated. The best clustering method hyperparameters were discovered using GridSearchCV.\n",
        "\n",
        "5.   **Cluster Visualization:** Scatter plots, word clouds, and 3D plots were used to visualise the clusters. These visualisations aided in comprehending the clusters' properties and distributions.\n",
        "6.   **Evaluation:** The quality of the clusters was evaluated using evaluation measures such the silhouette score, Calinski-Harabasz score, and Davies-Bouldin score. These measures gave us information about the clusters' separation, compactness, and similarity.\n",
        "\n",
        "Netflix's cluster project yielded valuable insights into its dataset, uncovering hidden patterns and grouping similar items. The analysis enabled personalized recommendations, targeted marketing, and content classification. Further refinement and exploration of algorithms, parameters, and features are needed to improve clustering performance and provide a foundation for data-driven decision making in the entertainment industry.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Description -\n",
        "1. **show_id :** Unique ID for every Movie / Tv Show\n",
        "\n",
        "2. **type :** Identifier - A Movie or TV Show\n",
        "\n",
        "3. **title :** Title of the Movie / Tv Show\n",
        "\n",
        "4. **director :** Director of the Movie\n",
        "\n",
        "5. **cast :** Actors involved in the movie / show\n",
        "\n",
        "6. **country :**  Country where the movie / show was produced\n",
        "\n",
        "7. **date_added :** Date it was added on Netflix\n",
        "\n",
        "8. **release_year :** Actual Releaseyear of the movie / show\n",
        "\n",
        "9. **rating :** TV Rating of the movie / show\n",
        "\n",
        "10. **dura tion :**Total Duration - in minutes or number of seasons\n",
        "\n",
        "11. **listed_in** :  Genere\n",
        "\n",
        "12. **description**: The Summary description"
      ],
      "metadata": {
        "id": "GrRxAc474u9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import math\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mtick\n",
        "from matplotlib.pyplot import figure\n",
        "import plotly.graph_objects as go\n",
        "import plotly.offline as py\n",
        "import plotly.express as px\n",
        "from datetime import datetime as dt\n",
        "import missingno as msno\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import re, string, unicodedata\n",
        "\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "string.punctuation\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import scipy.cluster.hierarchy as shc\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path='/content/drive/MyDrive/AlmaBetter/NETFLIX /Dataset/NETFLIX-MOVIES-AND-TV-SHOWS-CLUSTERING.csv'\n",
        "df=pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "ohsCLjUL4N2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Last View"
      ],
      "metadata": {
        "id": "Skij0S0Y8Osb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Last Look\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "tuUwuqqz8dQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "# Determining the count of duplicate rows in the DataFrame\n",
        "duplicate_rows = df[df.duplicated()]\n",
        "duplicate_count = len(duplicate_rows)\n",
        "\n",
        "# Printing the result\n",
        "print(\"Number of duplicate rows: \", duplicate_count)"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "null_counts = df.isnull().sum()/len(df)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.xticks(np.arange(len(null_counts)),null_counts.index,rotation='vertical')\n",
        "plt.ylabel('columns with missing data')\n",
        "plt.bar(np.arange(len(null_counts)),null_counts)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# null value distribution\n",
        "msno.bar(df, log=False, sort='ascending', figsize=(10,5), fontsize=12)"
      ],
      "metadata": {
        "id": "nZZbdLJZY6yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset contains descriptions of Netflix TV episodes, movies, and web\n",
        "series.\n",
        "\n",
        "There are 7787 rows and 12 columns.\n",
        "\n",
        "There are 11 categorical columns and 1 numerical column.\n",
        "\n",
        "There are no duplicate values present in dataset.\n",
        "\n",
        "Null values are present in director, cast, country, date_added, and rating; because date_added and rating have very few null values, we will delete them from the data."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "show_id : Unique ID for every Movie / Tv Show\n",
        "\n",
        "type : Identifier - A Movie or TV Show\n",
        "\n",
        "title : Title of the Movie / Tv Show\n",
        "\n",
        "director : Director of the Movie\n",
        "\n",
        "cast : Actors involved in the movie / show\n",
        "\n",
        "country : Country where the movie / show was produced\n",
        "\n",
        "date_added : Date it was added on Netflix\n",
        "\n",
        "release_year : Actual Releaseyear of the movie / show\n",
        "\n",
        "rating : TV Rating of the movie / show\n",
        "\n",
        "duration : Total Duration - in minutes or number of seasons\n",
        "\n",
        "listed_in : Genre\n",
        "\n",
        "description: The Summary description"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_counts = [df[col].nunique() for col in df.columns]\n",
        "columns = df.columns.tolist()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(columns, unique_counts, color='skyblue')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Number of Unique Values')\n",
        "plt.title('Number of Unique Values in Each Column')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1t4Qqb-UT9EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "#Tackle out the missing numbers\n",
        "#Chech he total numbers of Missing Number\n",
        "df.isnull().sum().sum()\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready\n",
        "\n",
        "# Handling the Null Values\n",
        "df['cast'].fillna(value='No cast',inplace=True)\n",
        "df['country'].fillna(value=df['country'].mode()[0],inplace=True)\n",
        "df['director'].fillna(value='NA',inplace=True)"
      ],
      "metadata": {
        "id": "jkg3SYkZUVMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'date_added' and 'rating' contribute only a small portion of the data, we will remove them from the dataset\n",
        "df.dropna(subset=['date_added','rating'],inplace=True)"
      ],
      "metadata": {
        "id": "zQYvYR8JUcqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for the null values after the data wrangling\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "YDt6c0kZUhu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing Values has been handled successfully"
      ],
      "metadata": {
        "id": "4moC-BYYxYMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manipulations:**\n",
        "1. Replaced missing values in the 'cast' column with the placeholder 'No cast'.\n",
        "\n",
        "2. Imputed the missing values in the 'country' column with the most frequently occurring value in the column (the mode).\n",
        "\n",
        "3. Removed rows containing null values in the 'date_added' and 'rating' columns.\n",
        "\n",
        "4. Verified if any null values still exist in the dataset.\n",
        "\n",
        "5. The Null values in Director column were filled with NA."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "After performing the manipulations on the dataset, some potential findings include:\n",
        "\n",
        "1. The predominant country listed in the 'country' column, after filling the null values, is likely to represent the most common country for Netflix content.\n",
        "\n",
        "2. The significance of the 'cast' column becomes evident as it required filling null values to maintain the dataset's completeness.\n",
        "\n",
        "3. The importance of the 'date_added' and 'rating' columns in the dataset may be questioned, as they were removed due to a limited number of null values. However, this judgment would rely on the specific analysis being conducted."
      ],
      "metadata": {
        "id": "SH3SRdl-U6SY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "\n",
        "# Setting the style\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "# Creating a new figure and axis\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Plotting the count using seaborn's countplot\n",
        "sns.countplot(x='type', data=df, palette='muted', ax=ax)\n",
        "\n",
        "# Labeling of values\n",
        "ax.set_title('Number of Movies and TV Shows', fontsize=14)\n",
        "ax.set_xlabel('Type', fontsize=12)\n",
        "ax.set_ylabel('Count', fontsize=12)\n",
        "\n",
        "# Visualization of number of movies and tv shows\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a countplot, a type of bar chart, is an excellent option for visually representing categorical data like the number of movies and TV shows on Netflix. This method effectively illustrates the frequency of each category in a straightforward and easily understandable manner."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix's library comprises 5372 movies and 2398 TV shows, indicating that the number of movies available surpasses the number of TV shows on the platform."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the fact that Netflix has more movies than TV shows may not significantly impact the business alone, it can be valuable when combined with other data to make informed decisions. For instance, if Netflix observes that its subscribers prefer TV shows over movies, it might prioritize acquiring more TV show content. Conversely, if their original movies gain popularity, they may invest more resources in that aspect.\n",
        "\n",
        "On the negative side, the specific observation about the movie-to-TV show ratio is unlikely to have adverse consequences by itself. However, disregarding subscriber preferences and persistently focusing on movies could lead to potential subscriber losses among those seeking more TV shows. Furthermore, if competitors begin offering more TV shows, Netflix's market share may suffer if it fails to respond by expanding its TV show content."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Displaying the rating column\n",
        "df['rating']\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new column target_ages\n",
        "ratings = {\n",
        "    'TV-PG': 'Older Kids',\n",
        "    'TV-MA': 'Adults',\n",
        "    'TV-Y7-FV': 'Older Kids',\n",
        "    'TV-Y7': 'Older Kids',\n",
        "    'TV-14': 'Teens',\n",
        "    'R': 'Adults',\n",
        "    'TV-Y': 'Kids',\n",
        "    'NR': 'Adults',\n",
        "    'PG-13': 'Teens',\n",
        "    'TV-G': 'Kids',\n",
        "    'PG': 'Older Kids',\n",
        "    'G': 'Kids',\n",
        "    'UR': 'Adults',\n",
        "    'NC-17': 'Adults'\n",
        "}\n",
        "\n",
        "df['target_ages'] = df['rating'].map(ratings)"
      ],
      "metadata": {
        "id": "mUoiE0n8snks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting 'type' column to categorical data type\n",
        "df['type'] = pd.Categorical(df['type'])\n",
        "\n",
        "# Creating a new categorical column 'target_ages' with specified categories\n",
        "df['target_ages'] = pd.Categorical(df['target_ages'], categories=['Kids', 'Older Kids', 'Teens', 'Adults'])\n"
      ],
      "metadata": {
        "id": "nNmYEXjEssEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "UEvbd1MOs_TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating two extra columns\n",
        "tv_shows=df[df['type']=='TV Show']\n",
        "movies=df[df['type']=='Movie']"
      ],
      "metadata": {
        "id": "FQRzZqrvtCXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping TV shows by 'rating' and counting the number of shows in each rating category\n",
        "tv_ratings = tv_shows.groupby(['rating'])['show_id'].count().reset_index(name='count').sort_values(by='count',ascending=False)\n",
        "\n",
        "# Setting figure dimensions\n",
        "fig_dims = (14,7)\n",
        "\n",
        "# Creating a figure and axis object with specified dimensions\n",
        "fig, ax = plt.subplots(figsize=fig_dims)\n",
        "\n",
        "# Creating a point plot using Seaborn's pointplot() function, with 'rating' on the x-axis and 'count' on the y-axis\n",
        "sns.pointplot(x='rating',y='count',data=tv_ratings)\n",
        "\n",
        "# Setting the plot title and font size\n",
        "plt.title('TV Show Ratings',size='20')\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pTpSZ0nEtFCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a color palette for the different target age groups\n",
        "colors = [\"#FFC300\", \"#FF5733\", \"#C70039\", \"#900C3F\"]\n",
        "\n",
        "# Plotting a countplot to show the movie ratings based on target age groups\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.title('Movie Ratings by Target Age Group')\n",
        "\n",
        "sns.countplot(x=movies['rating'], hue=movies['target_ages'], data=movies,\n",
        "              order=movies['rating'].value_counts().index, palette=colors)\n",
        "\n",
        "# Adding a legend to the plot\n",
        "plt.legend(title='Target Age Group', loc='upper right', labels=['Kids', 'Older Kids', 'Teens', 'Adults'])\n",
        "\n",
        "# Visualization of the chart\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jc8RYZMZtOtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason for selecting this chart is its ability to present the distribution of TV show ratings in a clear and succinct manner. The bars facilitate straightforward comparisons between various ratings, and the descending order by count accentuates the prevalence of TV-MA. As a result, this chart offers a swift and informative snapshot of the ratings landscape for TV shows on Netflix."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the dataset, TV-MA emerges as the most prevalent rating for both TV shows and movies on Netflix. This indicates a substantial portion of the platform's content is targeted towards adult audiences. The abundance of TV-MA ratings implies that Netflix's content predominantly caters to a mature demographic, featuring themes that may be intended for more mature viewers and potentially addressing controversial subjects."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the prevalence of TV-MA ratings can positively influence Netflix's business strategy. By recognizing the popularity of adult-oriented content, Netflix can continue to prioritize the production and acquisition of material that appeals to mature audiences. This strategy can help attract and retain subscribers interested in such themes while allowing the company to tailor its marketing efforts to specific age groups associated with different ratings.\n",
        "\n",
        "However, there is a potential downside to consider. Some subscribers might be dissuaded by the abundance of mature content, especially those seeking family-friendly programming. Consequently, there is a risk of losing subscribers who are not comfortable with adult themes. To maintain a balanced approach and prevent alienating specific demographics, Netflix should carefully curate its content offerings to cater to a wide range of viewers."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Extracting the count of movies and TV shows for each year\n",
        "movies_year = movies['release_year'].value_counts().sort_index(ascending=False)\n",
        "tvshows_year = tv_shows['release_year'].value_counts().sort_index(ascending=False)\n",
        "\n",
        "# Setting the style\n",
        "sns.set(style='whitegrid', font_scale=1.2)\n",
        "\n",
        "# Creating a new figure and axis\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "# Plotting the data using the axis object\n",
        "ax.plot(movies_year.index, movies_year.values, color='maroon', label='Movies', linewidth=2.5, marker='o')\n",
        "ax.plot(tvshows_year.index, tvshows_year.values, color='blue', label='TV Shows', linewidth=2.5, marker='o')\n",
        "\n",
        "# Customizing the plot\n",
        "plt.xticks(rotation=90)\n",
        "ax.set_xlabel('Release Year', fontsize=14)\n",
        "ax.set_ylabel('Number of Titles', fontsize=14)\n",
        "ax.set_title('Production Growth Yearly', fontsize=18, pad=15)\n",
        "ax.legend(fontsize=14)\n",
        "\n",
        "# Showing the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the last 20 years from the dataset\n",
        "last_20_years = range(2001, 2020)\n",
        "\n",
        "# Filtering the dataset to only include movies from the last 20 years\n",
        "movies_last_20_years = movies[movies['release_year'].isin(last_20_years)]\n",
        "\n",
        "# Setting the style\n",
        "sns.set_palette('mako')\n",
        "\n",
        "# Creating a new figure and axis\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Creating a count plot using the axis object\n",
        "sns.countplot(x='release_year', data=movies_last_20_years, order=last_20_years, ax=ax)\n",
        "\n",
        "# Customizing the plot\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "ax.set_xlabel('Year of Release')\n",
        "ax.set_ylabel('Number of Movies Released')\n",
        "ax.set_title('Number of Movies Released per Year in the Last 20 Years')\n",
        "\n",
        "# Showing the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PZ2CQNIkDou-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the tvshows_year column\n",
        "tvshows_year"
      ],
      "metadata": {
        "id": "I4GPkldLDuQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering for movies released in the last 15 years\n",
        "movies_last_15_years = df[df['release_year'] >= 2008]\n",
        "\n",
        "# Setting the style\n",
        "sns.set_palette('pastel')\n",
        "\n",
        "# Creating a new figure and axis\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Creating a count plot using the axis object with horizontal bars\n",
        "sns.countplot(y='release_year', data=movies_last_15_years, order=movies_last_15_years['release_year'].value_counts().index[:15], ax=ax)\n",
        "\n",
        "# Customizing the plot\n",
        "ax.set_title('Number of Movies Released per Year (2008-2022)', fontsize=16)\n",
        "ax.set_xlabel('Number of Movies')\n",
        "ax.set_ylabel('Release Year')\n",
        "\n",
        "# Showing the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uK2WFMwxD1ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the updated dataframe\n",
        "df"
      ],
      "metadata": {
        "id": "Rw9WAQN_D7Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding columns of month of addition\n",
        "\n",
        "df['month'] = pd.DatetimeIndex(df['date_added']).month\n",
        "df.head()"
      ],
      "metadata": {
        "id": "d7UVLCw_ECXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most suitable charts for displaying the number of movies and TV shows released per year from 2015 to 2020 would be a line chart or a bar chart. These charts allow for easy comparison between the two categories and help identify trends or patterns in the data. Another option could be a stacked bar chart or a stacked area chart, which can show the proportion of movies and TV shows released in each year.\n",
        "\n",
        "The reason for choosing these specific charts is that they effectively convey the message that the number of movies released on Netflix has been increasing at a faster rate than the number of TV shows. Moreover, the charts highlight the trend of increased production of both movies and TV shows after 2015, followed by a decline after 2020. Overall, these visualizations are instrumental in illustrating the growth and changes in Netflix's content over the specified years."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The years 2017 and 2018 saw the highest number of movie releases, while 2020 had the highest number of TV show releases.\n",
        "\n",
        "2. Movie releases on Netflix have grown at a significantly faster rate than TV show releases.\n",
        "\n",
        "3. Since 2015, there has been a substantial increase in the quantity of movies and TV show episodes available on Netflix.\n",
        "\n",
        "4. However, after 2020, there has been a noticeable decline in the number of movies and TV show episodes produced.\n",
        "\n",
        "5. It seems that Netflix has prioritized expanding its movie content over TV shows, given the much higher growth rate of movies compared to TV shows."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights derived from the analysis could positively impact Netflix's business strategy. The data indicates that focusing on increasing movie content could be a successful approach, attracting and retaining more viewers. However, the sharp decline in content production after 2020 may raise concerns for the company, suggesting possible challenges in production or insufficient investment in content creation. If this trend persists, negative growth could occur, prompting viewers to seek other streaming services with a broader content selection.\n",
        "\n",
        "In conclusion, while the analysis offers potential opportunities for Netflix, continuous monitoring of trends and adaptability to market changes are essential to ensure sustained growth and success."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Displaying the Distribution of Movie Releases by Month\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.set_palette('deep')\n",
        "sns.countplot(x='month', data=df)\n",
        "plt.title('Countplot of Month')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualization of Movie Releases by Month and Type\n",
        "fig, ax = plt.subplots(figsize=(15, 6))\n",
        "sns.set_palette('deep')\n",
        "\n",
        "sns.countplot(x='month', hue='type', data=df, edgecolor='black', linewidth=2.5, ax=ax)\n",
        "ax.set_title('Countplot of Movie Releases by Month and Type', fontsize=16)\n",
        "ax.set_xlabel('Month', fontsize=14)\n",
        "ax.set_ylabel('Count', fontsize=14)\n",
        "\n",
        "legend_labels = df['type'].unique()\n",
        "ax.legend(labels=legend_labels, fontsize=12, title='Type', title_fontsize=12)\n",
        "\n",
        "sns.despine()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ImtW65T7Eqx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chosen chart, a countplot with hue, is highly effective for visualizing and comparing the number of movies and TV shows added to Netflix each month. By using the hue parameter, we can clearly observe the individual contributions of movies and TV shows to the overall count for each month. This allows us to easily spot any patterns or trends in the data.\n",
        "\n",
        "Notably, the countplot reveals a significant surge in the addition of both movies and TV shows to Netflix from October to January. This information holds crucial importance for Netflix and content creators, as it indicates a period when audience interest in new content is likely to be at its peak. As a result, this time frame could be strategically advantageous for releasing new content to potentially maximize viewership and overall success."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the countplot, it is evident that Netflix experiences its peak in adding movies and TV shows during the months spanning from October to January. This timeframe appears to be the most active and bustling period for Netflix in terms of introducing fresh content to its platform."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Understanding that the majority of content is added to Netflix between October and January can potentially yield positive business outcomes. This information enables Netflix to strategize their content acquisition and release schedule effectively, maximizing user engagement during these months. For instance, they could focus on acquiring and releasing more popular titles during this period to attract and retain users.\n",
        "\n",
        "2. Nevertheless, it is crucial to acknowledge that relying solely on the countplot may not be enough to achieve a significant positive impact. To create a comprehensive content acquisition and release strategy, Netflix must analyze user viewing patterns, preferences, and closely monitor competition and market trends.\n",
        "\n",
        "3. In terms of negative growth, the countplot itself does not provide specific insights that lead to negative growth. However, if Netflix were to solely depend on this plot and overlook critical factors like user preferences, shifting market trends, and competitive landscape, it could potentially lead to negative growth due to an inadequate content selection and acquisition strategy."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Getting the top 10 genres\n",
        "top10_genres = movies['listed_in'].value_counts().nlargest(10)\n",
        "\n",
        "# Visualization of the top 10 genres\n",
        "plt.figure(figsize=(14, 6))\n",
        "top10_genres.plot(kind='barh')\n",
        "plt.title('Top 10 Genres of Movies', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Count', fontsize=14)\n",
        "plt.ylabel('Genre', fontsize=14)\n",
        "plt.gca().invert_yaxis()  # Inverting the y-axis to display genres from top to bottom\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the top 10 genres\n",
        "top10_genres_tvshows = tv_shows['listed_in'].value_counts().nlargest(10)\n",
        "\n",
        "# Visualization of the top 10 genres\n",
        "plt.figure(figsize=(14, 6))\n",
        "top10_genres_tvshows.plot(kind='barh')\n",
        "plt.title('Top 10 Genres of TV Shows', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Count', fontsize=14)\n",
        "plt.ylabel('Genre', fontsize=14)\n",
        "plt.gca().invert_yaxis()  # Inverting the y-axis to display genres from top to bottom\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lHELeopSFypc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix offers a wide variety of TV show genres, but one genre that consistently captures the hearts of viewers of all ages is kids TV. The kids TV category on Netflix presents a diverse collection of animated and live-action shows, catering to families seeking enjoyable and educational content. Classic favorites like SpongeBob SquarePants and Power Rangers coexist with fresh and captivating series such as Carmen Sandiego and The Dragon Prince, ensuring there's something for every young viewer.\n",
        "\n",
        "What sets Netflix's kids TV category apart is its commitment to providing a safe and secure viewing environment for children. Parents can utilize features like parental controls to set age-appropriate filters, monitor viewing history, and restrict access to specific shows or movies, ensuring a worry-free entertainment experience for the whole family.\n",
        "\n",
        "Whether you need entertaining options for a rainy day or seek a way to connect with your loved ones through shared TV experiences, Netflix's kids TV category offers an extensive selection of entertaining and enriching content, making it a popular choice among users."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix offers a wide variety of TV show genres, but one genre that consistently captures the hearts of viewers of all ages is kids TV. The kids TV category on Netflix presents a diverse collection of animated and live-action shows, catering to families seeking enjoyable and educational content. Classic favorites like SpongeBob SquarePants and Power Rangers coexist with fresh and captivating series such as Carmen Sandiego and The Dragon Prince, ensuring there's something for every young viewer.\n",
        "\n",
        "What sets Netflix's kids TV category apart is its commitment to providing a safe and secure viewing environment for children. Parents can utilize features like parental controls to set age-appropriate filters, monitor viewing history, and restrict access to specific shows or movies, ensuring a worry-free entertainment experience for the whole family.\n",
        "\n",
        "Whether you need entertaining options for a rainy day or seek a way to connect with your loved ones through shared TV experiences, Netflix's kids TV category offers an extensive selection of entertaining and enriching content, making it a popular choice among users."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The most popular genre among TV shows on Netflix is kids TV, featuring a mix of educational and entertaining content suitable for children of all ages. This category includes well-known series like \"Paw Patrol,\" \"Peppa Pig,\" \"The Magic School Bus,\" and \"Stranger Things.\"\n",
        "\n",
        "2. The valuable insights derived from this information can have a positive impact on Netflix's business. With a clear understanding of their audience's preferences, Netflix can customize their content offerings and marketing strategies to better resonate with their target viewers. For instance, they might allocate more resources to producing high-quality kids shows and actively promote them to parents with young kids.\n",
        "\n",
        "3. However, this trend could also present some challenges for growth. If Netflix excessively prioritizes kids TV shows at the expense of other genres, they may risk alienating older viewers who seek more mature content. Additionally, any decline in the quality of their kids programming or the loss of rights to popular shows could potentially harm their business. Striking a balance between catering to their core audience and offering a diverse range of content is crucial for sustaining broader appeal."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Creating a figure and setting it's size\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Extracting the duration values as integers using regex and plotting a histogram\n",
        "sns.histplot(movies['duration'].str.extract('(\\d+)').astype(int), kde=False, color='red')\n",
        "\n",
        "# Setting the title of the plot\n",
        "plt.title('Distribution of Movie Durations', fontweight='bold')\n",
        "\n",
        "# Setting the x-axis label\n",
        "plt.xlabel('Duration (minutes)')\n",
        "\n",
        "# Setting the y-axis label\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Showing the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the figure size\n",
        "plt.figure(figsize=(30, 6))\n",
        "\n",
        "# Creating a count plot of TV Show durations\n",
        "sns.countplot(x=tv_shows['duration'], data=tv_shows, order=tv_shows['duration'].value_counts().index)\n",
        "\n",
        "# Setting the title of the plot\n",
        "plt.title(\"Distribution of TV Show Durations\", fontweight='bold',size='30')\n",
        "\n",
        "# Setting the x-axis label\n",
        "plt.xlabel(\"Duration (seasons)\",size='20')\n",
        "\n",
        "# Setting the y-axis label\n",
        "plt.ylabel(\"Count\",size='20')\n",
        "\n",
        "# Rotating the x-axis labels\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Showing the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g9Tsz7qSImoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the duration values as integers using regex\n",
        "movies['minute'] = movies['duration'].str.extract('(\\d+)').apply(pd.to_numeric)\n",
        "\n",
        "# Calculating the average movie duration by rating\n",
        "duration_year = movies.groupby(['rating'])['minute'].mean()\n",
        "\n",
        "# Creating a DataFrame to store the results and sort by average duration\n",
        "duration_df = pd.DataFrame(duration_year).sort_values('minute')\n",
        "\n",
        "# Setting the figure size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Creating a bar plot of the average movie duration by rating\n",
        "ax = sns.barplot(x=duration_df.index, y=duration_df.minute)\n",
        "\n",
        "# Setting the title of the plot\n",
        "plt.title(\"Average Movie Duration by Rating\", fontweight='bold')\n",
        "\n",
        "# Setting the x-axis label\n",
        "plt.xlabel(\"Rating\")\n",
        "\n",
        "# Setting the y-axis label\n",
        "plt.ylabel(\"Average Duration (minutes)\")\n",
        "\n",
        "# Showing the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XAU0i-VqIs2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The correlation between movie duration and rating plays a significant role in influencing a viewer's movie-watching decision. Visualizing this relationship through a chart helps to identify patterns and trends that can be insightful for various stakeholders. For instance, the chart's findings indicate that NC-17 movies generally have longer runtimes compared to other ratings, which could be beneficial information for filmmakers and movie studios.\n",
        "\n",
        "Likewise, the chart highlights that TV-Y rated movies tend to be shorter, making it valuable for parents seeking age-appropriate content for their children. Such a chart serves as a valuable resource for filmmakers, studios, distributors, and viewers, offering useful insights into the movie industry's dynamics and aiding in making informed decisions."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon analyzing movie durations, it was evident that a significant portion of movies falls within the 50 to 150-minute range. Conversely, for TV shows, there is a notable prevalence of single-season shows, indicating a predominance of relatively new content in the TV category on Netflix.\n",
        "\n",
        "The analysis also highlighted that movies with an NC-17 rating exhibit the longest average duration. This observation can be attributed to the fact that movies with this rating often delve into mature themes and explicit content, necessitating a longer runtime to effectively portray the story.\n",
        "\n",
        "Conversely, movies with a TV-Y rating, suitable for all children, have the shortest average runtime. This suggests that movies with this rating are typically shorter and may revolve around simpler plots and themes that cater to younger audiences."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The insights gained from the analysis hold the potential to create a positive business impact, empowering movie studios and streaming platforms to gain a deeper understanding of their audience's preferences. This understanding enables them to tailor their content offerings to meet the specific needs and interests of their viewers. For example, noticing that NC-17 rated movies tend to have longer runtimes may lead to increased investment in creating mature content for adult audiences. Similarly, recognizing that TV-Y rated movies have shorter runtimes may drive the focus towards producing engaging, family-friendly content for younger viewers.\n",
        "\n",
        "2. However, there is also the possibility of negative growth stemming from certain insights. For instance, if studios and platforms observe that most TV shows only consist of a single season, they may become reluctant to invest in additional seasons for a show, even if it has a dedicated fanbase. This decision could lead to limited growth in terms of audience and revenue for particular shows or franchises. Additionally, if they discover that movies with certain ratings consistently perform poorly in terms of ratings or box office revenue, they might avoid investing in similar projects in the future. This cautious approach may, in turn, reduce the variety of content available to audiences."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Set the figure size to 18x5\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "# Create a countplot for the 'country' column\n",
        "# Order the bars in descending order by their value counts\n",
        "# Limit the plot to show only the top 15 countries\n",
        "# Use different colors for 'TV Show' and 'Movie' categories\n",
        "sns.countplot(x=df['country'], order=df['country'].value_counts().index[0:15], hue=df['type'])\n",
        "\n",
        "# Rotate the x-axis tick labels by 50 degrees for better visibility\n",
        "plt.xticks(rotation=50)\n",
        "\n",
        "# Set the plot title with larger font size and bold text\n",
        "plt.title('Top 15 Countries with Most Content', fontsize=15, fontweight='bold')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Count the occurrences of each country in the 'country' column and reset the index\n",
        "# This will give us a DataFrame with two columns: 'country' containing country names and 'count' containing the counts\n",
        "country_counts = df['country'].value_counts().reset_index()\n",
        "\n",
        "# Rename the columns to provide more clarity\n",
        "country_counts.columns = ['Country', 'Count']\n",
        "\n",
        "# Display the DataFrame showing the count of contents in each country\n",
        "country_counts\n"
      ],
      "metadata": {
        "id": "I4tY_TpKNDWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the top 10 countries by count of titles and store them in 'country_order'\n",
        "country_order = df['country'].value_counts()[:10].index\n",
        "\n",
        "# Create a new DataFrame 'content_data' with count of movies and TV shows for each country in 'country_order'\n",
        "content_data = df[['type', 'country']].groupby('country')['type'].value_counts().unstack().loc[country_order]\n",
        "\n",
        "# Add a new column 'total' to 'content_data' representing the total count of titles for each country\n",
        "content_data['total'] = content_data.sum(axis=1)\n",
        "\n",
        "# Calculate the ratio of movies and TV shows for each country and store it in 'content_data_ratio'\n",
        "content_data_ratio = (content_data.T / content_data['total']).T[['Movie', 'TV Show']]\n",
        "\n",
        "# Create a horizontal bar chart to visualize the data, with stacked bars representing the movie and TV show ratio\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "content_data_ratio.sort_values(by='Movie').plot(kind='barh', stacked=True, ax=ax)\n",
        "\n",
        "# Set the x-axis label and plot title\n",
        "ax.set_xlabel('Ratio of Titles', fontsize=14)\n",
        "ax.set_title('Ratio of Movies and TV Shows by Country', fontsize=18)\n",
        "\n",
        "# Set the legend for the plot\n",
        "# Reversed order of handles and labels to match the bar colors with the legend correctly\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "ax.legend(reversed(handles), reversed(labels), fontsize=12, loc='upper right')\n"
      ],
      "metadata": {
        "id": "VG-IyqWANH77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Add a new column 'count' to the DataFrame to indicate the presence of each title (set to 1)\n",
        "df['count'] = 1\n",
        "\n",
        "# Group the DataFrame by 'country', calculate the sum of 'count' for each country,\n",
        "# then sort the data in descending order based on the total count\n",
        "# and store the top 10 countries with the highest counts in 'data' DataFrame\n",
        "data = df.groupby('country')['count'].sum().sort_values(ascending=False).reset_index()[:10]\n",
        "top_countries = data['country']  # Extract the names of the top 10 countries\n",
        "\n",
        "# Create a new DataFrame 'df_heatmap' containing data only for the top 10 countries\n",
        "df_heatmap = df[df['country'].isin(top_countries)]\n",
        "\n",
        "# Create a pivot table using 'pd.crosstab' to calculate the relative frequency of content target ages\n",
        "# for each country and normalize it by index (rows sum up to 1)\n",
        "df_heatmap = pd.crosstab(df_heatmap['target_ages'], df_heatmap['country'], normalize=\"index\")\n",
        "\n",
        "# Transpose the pivot table to switch rows and columns, making it suitable for a heatmap\n",
        "df_heatmap = df_heatmap.T\n",
        "\n",
        "# Display the resulting DataFrame 'df_heatmap' ready for the heatmap\n",
        "df_heatmap\n"
      ],
      "metadata": {
        "id": "Pb4WOMatNNib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the provided data, it is evident that Netflix has the most extensive content library in the United States, closely followed by India. Furthermore, India stands out as the country with the highest number of movies available on the platform.\n",
        "\n",
        "To effectively convey this information visually, a bar chart or a horizontal bar chart would be suitable choices. A bar chart can display the title count for each country side by side, facilitating easy comparison. Alternatively, a horizontal bar chart can also be effective, especially if we wish to present the countries in descending order of title count."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The United States leads with the highest content count on Netflix, with India following closely behind. Additionally, India boasts the highest number of movies available on the platform."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Our analysis indicates that the United States leads with the highest number of content on Netflix, with India ranking second in terms of content volume.\n",
        "\n",
        "2. Interestingly, India stands out with the highest count of movies available on the Netflix platform.\n",
        "\n",
        "3. These findings offer valuable insights for Netflix to optimize their content recommendations for users based on their geographic locations.\n",
        "\n",
        "4. Additionally, this information can aid Netflix in strategizing their content production efforts, focusing on content types that resonate well with specific countries.\n",
        "\n",
        "5. However, it's crucial to consider potential drawbacks. Overemphasizing content production for certain regions may lead to neglecting other markets, impacting viewership and revenue.\n",
        "\n",
        "6. Moreover, if Netflix heavily favors producing one content type, they might miss out on opportunities to attract viewers who prefer other content genres, potentially limiting their audience reach.\n",
        "\n",
        "In conclusion, while the analysis provides valuable insights to guide Netflix's decisions, it is essential to approach these insights with a balanced perspective, acknowledging both positive and negative impacts.\n",
        "\n"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Convert the 'date_added' column in the DataFrame 'df' to datetime format\n",
        "# This allows us to work with the date as a datetime object, enabling various date-based operations\n",
        "df['date_added'] = pd.to_datetime(df['date_added'])\n",
        "\n",
        "# Extract the year from the 'date_added' column and create a new column 'year_added' in the DataFrame 'df'\n",
        "# This column will hold the year information for each title's date of addition on Netflix\n",
        "movies['year_added'] = df['date_added'].dt.year\n",
        "\n",
        "# Display the updated DataFrame 'df' with the converted 'date_added' column and the new 'year_added' column\n",
        "df\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column 'originals' in the 'movies' DataFrame to indicate if a movie is an original or not\n",
        "# If the 'release_year' and 'year_added' values are the same, it means the movie is an original, otherwise, it's not\n",
        "movies['originals'] = np.where(movies['release_year'] == movies['year_added'], 'Yes', 'No')\n",
        "\n",
        "# Create a pie chart to visualize the percentage of original movies and non-original movies in the dataset\n",
        "fig, ax = plt.subplots(figsize=(5, 5), facecolor=\"#363336\")\n",
        "ax.patch.set_facecolor('#363336')\n",
        "\n",
        "# Specify the 'explode' parameter to create some separation between the slices\n",
        "explode = (0, 0.1)\n",
        "\n",
        "# Count the number of movies in each category (originals and others) using 'value_counts()'\n",
        "# Plot a pie chart with labels, percentages, and shadows using the 'ax.pie()' method\n",
        "# Change the colors of the pie chart slices to more attractive colors\n",
        "ax.pie(movies['originals'].value_counts(), explode=explode, autopct='%.2f%%', labels=['Others', 'Originals'],\n",
        "       shadow=True, startangle=90, textprops={'color': \"black\", 'fontsize': 20},\n",
        "       colors=['#ff9999', '#66b3ff'])\n",
        "\n",
        "# Set the title for the plot\n",
        "ax.set_title(\"Percentage of Originals vs Others in Movies\", color='white', fontsize=20)\n",
        "\n",
        "# Display the pie chart\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mYZZAVmKQgrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pie chart was used in the provided code to visually represent the percentage distribution of original movies and non-original movies in the dataset. Pie charts are commonly used to display how a single whole (100%) is divided into various parts, showing the proportion of each part relative to the whole."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite Netflix's reputation for producing original content, it is intriguing to discover that only 30% of the movies on the platform are actual Netflix productions. The majority, around 70%, consists of movies that were acquired from various sources, including theaters and other streaming platforms.\n",
        "\n",
        "This statistic emphasizes the vast collection of films that Netflix has amassed over time, offering viewers a diverse array of content from different parts of the world. From classic Hollywood movies to foreign cinema, Netflix caters to a wide range of interests and preferences.\n",
        "\n",
        "As you navigate through Netflix's extensive movie library, it's essential to remember that only a small fraction of it constitutes original content. The majority of the movies available have been procured and added to the platform, providing viewers with an abundance of entertainment choices."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Looking at the positive aspect, the fact that 70% of movies added to Netflix were previously released through other modes showcases Netflix's ability to acquire popular content that has already proven successful. This strength enables Netflix to offer a diverse selection of content to its customers without bearing the high expenses of producing original content.\n",
        "\n",
        "2. Furthermore, the presence of 30% of movies that are Netflix's own original productions highlights their investment in creating exclusive content. This strategic move allows Netflix to stand out from competitors and create distinctive material that attracts new viewers while retaining existing ones.\n",
        "\n",
        "However, considering the negative side, if Netflix struggles to produce original content as appealing as the acquired content, it might experience a decline in its subscriber base. Additionally, relying too heavily on acquired content could lead to challenges in negotiating favorable licensing agreements with content providers, potentially resulting in increased costs and reduced profitability.\n",
        "\n"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "# Correlation Heatmap visualization code\n",
        "\n",
        "# Create a new column 'count' in the DataFrame to indicate the presence of each title (set to 1)\n",
        "df['count'] = 1\n",
        "\n",
        "# Group the DataFrame by 'country', calculate the sum of 'count' for each country,\n",
        "# then sort the data in descending order based on the total count\n",
        "# and store the top 10 countries with the highest counts in 'data' DataFrame\n",
        "data = df.groupby('country')['count'].sum().sort_values(ascending=False).reset_index()[:10]\n",
        "top_countries = data['country']  # Extract the names of the top 10 countries\n",
        "\n",
        "# Create the DataFrame 'df_heatmap' containing data only for the top 10 countries\n",
        "df_heatmap = df[df['country'].isin(top_countries)]\n",
        "\n",
        "# Create a pivot table using 'pd.crosstab' to calculate the relative frequency of content target ages\n",
        "# for each country and normalize it by index (rows sum up to 1)\n",
        "df_heatmap = pd.crosstab(df_heatmap['target_ages'], df_heatmap['country'], normalize=\"index\")\n",
        "\n",
        "# Transpose the pivot table to switch rows and columns, making it suitable for a heatmap\n",
        "df_heatmap = df_heatmap.T\n",
        "\n",
        "# Display the resulting DataFrame 'df_heatmap' ready for the heatmap\n",
        "df_heatmap"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a heatmap to visualize the relative frequency of content target ages for the top 9 countries\n",
        "# 'country_order2' specifies the order of countries to be displayed on the heatmap\n",
        "# 'age_order' specifies the order of content target ages to be displayed on the heatmap\n",
        "# We use 'df_heatmap' directly without specifying rows using 'loc'\n",
        "\n",
        "# Set up the figure and axes for the heatmap, with a size of 12x12\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
        "\n",
        "# Define the order of countries to display on the heatmap\n",
        "country_order2 = ['United States', 'India', 'United Kingdom', 'Canada', 'Japan', 'France', 'South Korea', 'Spain', 'Mexico']\n",
        "\n",
        "# Define the order of content target ages to display on the heatmap\n",
        "age_order = ['Adults', 'Teens', 'Older Kids', 'Kids']\n",
        "\n",
        "# Create the heatmap using 'sns.heatmap()' with the specified parameters\n",
        "# 'cmap' sets the color map to 'YlGnBu' for a yellow-green-blue gradient\n",
        "# 'square' ensures that the heatmap cells are square\n",
        "# 'linewidth' sets the width of the lines between cells to 2.5\n",
        "# 'cbar' is set to False to hide the color bar\n",
        "# 'annot' is set to True to display annotations (values) on the heatmap cells\n",
        "# 'fmt' specifies the format of the annotations as '1.0%' to show percentages without decimals\n",
        "# 'vmax' and 'vmin' set the upper and lower limits for the color map to enhance visibility\n",
        "# 'annot_kws' is used to set the font size for annotations\n",
        "sns.heatmap(df_heatmap, cmap=\"YlGnBu\", square=True, linewidth=2.5, cbar=False,\n",
        "            annot=True, fmt='1.0%', vmax=.6, vmin=0.05, ax=ax, annot_kws={\"fontsize\": 12})\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7j-1TDG9UWJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap was used to visualize the relative frequency of content target ages for the top countries in the Netflix dataset. Heatmaps are particularly effective for displaying the correlation between two categorical variables. In this case, the columns represent different content target ages (e.g., Adults, Teens, Older Kids, Kids), and the rows represent the top countries (e.g., United States, India, United Kingdom, etc.)."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The content target ages on Netflix for the United States and the United Kingdom are similar, showing a close alignment between the two countries. However, when compared to countries like India or Japan, their content preferences differ significantly.\n",
        "\n",
        "Moreover, Mexico and Spain share similar content on Netflix across various age groups, indicating a similarity in content choices between these two countries despite their distinct geographic locations."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothetical Statement - Netflix has the highest number of content in the United States, followed by India. India has the highest number of movies on Netflix.**"
      ],
      "metadata": {
        "id": "65JGrFURo2la"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. State Your research hypothesis as a null hypothesis and alternate hypothesis.\n",
        "\n",
        "**Null hypothesis (H0):** The average number of movies on Netflix in the United States is equal to the average number of movies on Netflix in India.\n",
        "\n",
        "**Alternative hypothesis (H1):** The average number of movies on Netflix in the United States is greater than the average number of movies on Netflix in India.."
      ],
      "metadata": {
        "id": "cPKHf5m0o-EO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "gLhN-EM0pXj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Step 1: Filter movies only from the DataFrame 'df'\n",
        "movies = df[df.type == 'Movie']\n",
        "\n",
        "# Step 2: Filter movies by country (United States and India)\n",
        "us_movies = movies[movies.country == 'United States']\n",
        "india_movies = movies[movies.country == 'India']\n",
        "\n",
        "# Step 3: Perform the t-test to compare the release years of movies in the US and India\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# We use the independent two-sample t-test since we are comparing the means of two groups.\n",
        "# The assumption 'equal_var=False' means we are not assuming equal variances between the groups.\n",
        "t_statistic, p_value = ttest_ind(us_movies['release_year'], india_movies['release_year'], equal_var=False)\n",
        "\n",
        "# Step 4: Print the results\n",
        "alpha = 0.05  # significance level\n",
        "\n",
        "# Check if the p-value is less than the significance level to determine the result of the t-test.\n",
        "if p_value < alpha:\n",
        "    print(\"Reject null hypothesis. The average release year of movies on Netflix in the United States is greater than the average release year in India.\")\n",
        "else:\n",
        "    print(\"Fail to reject null hypothesis. The average release year of movies on Netflix in the United States is equal to the average release year in India.\")\n"
      ],
      "metadata": {
        "id": "QQQVJrOOpjFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which statistical test have you done to obtain P-Value?\n",
        "\n",
        "\n",
        "The p-value was obtained using a two-sample t-test, also known as an independent samples t-test or unpaired t-test. The ttest_ind function from the scipy.stats module was utilized for this purpose. This test is suitable for comparing the means of two independent samples, which in this case involves comparing the number of movies on Netflix between the United States and India.\n",
        "\n",
        "Importantly, the assumption was made that the variances of the two populations are not equal (equal_var=False was set in the ttest_ind function). This decision was reasonable since there may be differences in the variances of the number of movies on Netflix in the United States and India. However, if there were reasons to believe that the variances were equal, a pooled t-test could have been used instead."
      ],
      "metadata": {
        "id": "A7sCH4_Lu3mw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why did you choose the specific statistical test?\n",
        "\n",
        "1. The reason for choosing the two-sample t-test in this analysis is its suitability for comparing the means of two independent samples. In this case, we have two distinct samples of movies on Netflix, one from the United States and another from India, and our objective is to determine if the average number of movies in the United States significantly differs from that in India.\n",
        "\n",
        "2. The t-test is well-suited because the population standard deviations are unknown, and we are working with relatively small sample sizes compared to the total number of movies on Netflix. Hence, using the sample standard deviations to estimate the population standard deviations is necessary.\n",
        "\n",
        "3. Moreover, the t-test assumes that the data follows a normal distribution or is approximately normally distributed. This assumption is reasonable considering the nature of the data being analyzed.\n",
        "\n",
        "All in all, the two-sample t-test is a widely accepted and dependable statistical test for comparing means of two independent samples, making it a sound choice for this particular analysis."
      ],
      "metadata": {
        "id": "CMDv14Yvx6wA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothetical Statement -**\n",
        " According to the countplot, it appears that Netflix adds the highest number of movies and TV shows during the period between October and January. This period seems to be the busiest time of year for Netflix in terms of adding new content to its platform.\n",
        "1. State Your research hypothesis as a null hypothesis and alternate hypothesis.\n",
        "\n",
        "Null hypothesis(H0): There is no significant difference in the number of movies and TV shows added by Netflix across different months.\n",
        "\n",
        "Alternative hypothesis(H1): There is a significant difference in the number of movies and TV shows added by Netflix across different months."
      ],
      "metadata": {
        "id": "I2K8qQBryJms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "1H1TU6g8ytoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Import the required libraries\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Convert the \"date_added\" column to datetime format\n",
        "df[\"date_added\"] = pd.to_datetime(df[\"date_added\"])\n",
        "\n",
        "# Extract the month from the \"date_added\" column and create a new column 'month_added'\n",
        "df[\"month_added\"] = df[\"date_added\"].dt.month_name()\n",
        "\n",
        "# Create a contingency table to count the number of new movies and TV shows added by month\n",
        "contingency_table = pd.crosstab(df[\"type\"], df[\"month_added\"])\n",
        "\n",
        "# Perform a chi-square test for independence\n",
        "chi2_statistic, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
        "\n",
        "# Output the results\n",
        "print(\"Chi-square statistic:\", chi2_statistic)\n",
        "print(\"P-value:\", p_value)"
      ],
      "metadata": {
        "id": "CNcL0pGIyBYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which statistical test have you done to obtain P-Value?\n",
        "\n",
        "The p-value was obtained through a chi-square test for independence, which is used to assess whether there is a meaningful connection between two categorical variables. In this analysis, we aimed to determine if there is a significant association between the time of year and the number of new movies and TV shows added to Netflix. The test involves comparing the actual frequencies of occurrences in a contingency table (which presents the data's distribution) to the expected frequencies based on the assumption of independence.\n",
        "\n",
        "To compute the test statistic, the squared differences between observed and expected frequencies are summed up, and this statistic follows a chi-square distribution. By calculating the p-value, we ascertain the probability of obtaining a test statistic as extreme or more extreme than the observed one, under the assumption that the null hypothesis (indicating independence) is true. A p-value lower than the chosen significance level (typically 0.05) leads us to reject the null hypothesis and conclude that a significant association exists between the two categorical variables being examined."
      ],
      "metadata": {
        "id": "OeA0DxTKqU69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why did you choose the specific statistical test?\n",
        "\n",
        "1. The chi-square test for independence was chosen to investigate any potential relationship between the time of year and the number of new Netflix movies and TV shows.\n",
        "\n",
        "2. This test is suitable for analyzing categorical variables, comparing observed and expected distributions under the assumption of independence.\n",
        "\n",
        "3. The resulting p-value helps determine if there is significant evidence to reject the null hypothesis and conclude whether an association between the two variables exists."
      ],
      "metadata": {
        "id": "xHdNrkbEy-tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothetical Statement** - The number of movies on Netflix is greater than the number of TV shows, with 5372 movies and 2398 TV shows currently available on the platform."
      ],
      "metadata": {
        "id": "F5iWQ0zOzM8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. State Your research hypothesis as a null hypothesis and alternate hypothesis.\n",
        "\n",
        "Null hypothesis(H0): The number of movies and TV shows on Netflix is not significantly different.\n",
        "\n",
        "Alternative hypothesis(H1): The number of movies on Netflix is significantly greater than the number of TV shows."
      ],
      "metadata": {
        "id": "i32Nbt0tzSG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "_-i5ON692GR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Import the required library\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "# Count the number of movies and TV shows\n",
        "n_movies = df[df['type'] == 'Movie'].count()['type']\n",
        "n_tv_shows = df[df['type'] == 'TV Show'].count()['type']\n",
        "\n",
        "# Set up the counts and sample sizes for the z-test\n",
        "counts = [n_movies, n_tv_shows]\n",
        "nobs = [len(df), len(df)]\n",
        "\n",
        "# Perform the z-test assuming equal proportions (null hypothesis: proportion of movies and TV shows is equal)\n",
        "z_stat, p_val = proportions_ztest(counts, nobs, value=0, alternative='larger')\n",
        "\n",
        "# Output the results\n",
        "print('Number of movies:', n_movies)\n",
        "print('Number of TV shows:', n_tv_shows)\n",
        "print('z-statistic:', z_stat)\n",
        "print('p-value:', p_val)"
      ],
      "metadata": {
        "id": "V7kV1C6J17mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which statistical test have you done to obtain P-Value?\n",
        "\n",
        "A two-sample z-test for proportions was employed to obtain the p-value. The test aimed to assess whether the proportion of movies and TV shows on Netflix is equal (null hypothesis) or if the proportion of movies is greater than the proportion of TV shows (alternative hypothesis). The proportions_ztest() function from the statsmodels library was used for this test, calculating the z-score and the p-value based on the sample proportions, sample sizes, and the specified null hypothesis value."
      ],
      "metadata": {
        "id": "uyjWHCK_2UMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why did you choose the specific statistical test?\n",
        "\n",
        "1. The two-sample z-test for proportions was chosen to compare the number of movies and TV shows on Netflix due to the categorical nature of the data. The objective was to determine if there is a significant difference in the proportions of these categories in the overall population.\n",
        "\n",
        "2. This test is appropriate when dealing with two independent samples and comparing the proportion of successes in each sample, where success corresponds to either a movie or a TV show.\n",
        "\n",
        "3. The z-test assumes that the sample sizes are large enough to apply the normal approximation to the binomial distribution, which is valid in this case, given the sizable sample size, enabling us to effectively test the hypothesis of interest."
      ],
      "metadata": {
        "id": "qD0PNWxr2bvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Calculate the number of missing values in each column using the isna() method.\n",
        "missing_count = df.isna().sum()\n",
        "\n",
        "# Step 2: Calculate the total number of rows in the DataFrame.\n",
        "total_rows = len(df)\n",
        "\n",
        "# Step 3: Calculate the percentage of missing data for each column.\n",
        "missing_percentage = (missing_count / total_rows) * 100\n",
        "\n",
        "# Step 4: Round the percentages to two decimal places using the round() function.\n",
        "rounded_percentage = missing_percentage.round(2)\n",
        "\n",
        "# Step 5: Sort the percentages in descending order to find the columns with the most missing data.\n",
        "sorted_percentage = rounded_percentage.sort_values(ascending=False)\n",
        "\n",
        "# Step 6: Print the result, which shows the percentage of missing data for each column in descending order.\n",
        "print(sorted_percentage)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Fill missing values in the 'director', 'cast', and 'country' columns with empty strings.\n",
        "df[['director', 'cast', 'country']] = df[['director', 'cast', 'country']].fillna(' ')\n",
        "\n",
        "# Step 2: Drop rows with any remaining missing values after filling the above columns.\n",
        "df.dropna(axis=0, inplace=True)\n"
      ],
      "metadata": {
        "id": "bJ0_Fsy058rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Check for null values in the DataFrame after treating missing values.\n",
        "null_counts_after_treatment = df.isna().sum()\n",
        "\n",
        "# Step 2: Print the result to see if there are any null values in each column.\n",
        "print(null_counts_after_treatment)\n"
      ],
      "metadata": {
        "id": "aQOPIEW759om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import the necessary libraries.\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 2: Create a boxplot to visualize the distribution of the data and detect outliers.\n",
        "#          The boxplot helps identify data points that are far from the central distribution.\n",
        "#          Outliers are the points that lie beyond the \"whiskers\" of the boxplot.\n",
        "#          Seaborn's boxplot function is used, and the DataFrame 'df' is passed as the data.\n",
        "sns.boxplot(data=df)\n",
        "\n",
        "# Step 3: Display the plot.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9ixDSQnU6Ok1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatment\n",
        "\n",
        "# Step 1: Create a figure with two axes (subplots) to display the distribution plot and boxplot side by side.\n",
        "#          'fig' represents the entire figure, and 'ax' contains the two subplots (1 row, 2 columns).\n",
        "#          The figure size is set to (15, 5) using 'figsize'.\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Step 2: Plot the distribution plot on the first subplot (ax[0]).\n",
        "#          The distribution plot helps visualize the data's frequency distribution.\n",
        "#          We use Seaborn's 'distplot' function to create the distribution plot for the 'release_year' column of 'df'.\n",
        "sns.distplot(x=df['release_year'], ax=ax[0])\n",
        "\n",
        "# Step 3: Plot the boxplot on the second subplot (ax[1]).\n",
        "#          The boxplot helps identify outliers in the 'release_year' column.\n",
        "#          We use Seaborn's 'boxplot' function to create the boxplot for the entire DataFrame 'df'.\n",
        "sns.boxplot(data=df, ax=ax[1])\n",
        "\n",
        "\n",
        "# Step 4: Display the plots.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. With the exception of the release year, the majority of the data is provided in textual form.\n",
        "\n",
        "2. The text-based format contains all the necessary data for constructing a clustering or predictive model, rendering outlier handling unnecessary."
      ],
      "metadata": {
        "id": "FtlzceEHBjcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Textual Data Preprocessing**\n",
        "**What is Textual Data Preprocessing ?**\n",
        "\n",
        "Textual data preprocessing refers to the set of actions taken to get text data ready for analysis or modeling purposes. These steps involve cleaning, organizing, and standardizing raw text, making it suitable for natural language processing or machine learning models. The process includes tokenization, stop-word removal, stemming or lemmatization, converting to lowercase, eliminating punctuation, and getting rid of numbers. The ultimate objective of textual data preprocessing is to make the data more suitable for analysis and modeling by removing irrelevant details and standardizing the text format. This enhances the accuracy and efficiency of subsequent analyses and modeling tasks.\n",
        "\n",
        "**Modeling Approch**\n",
        "\n",
        "1. Similar to organizing a cluttered closet by grouping items with similar attributes, clustering is a technique used to group similar data points together. In this case, we apply clustering to a set of movies to discover patterns and categorize them based on their attributes.\n",
        "\n",
        "2. Before performing clustering, we need to preprocess the textual data, just like sorting clothes by color or size. This involves techniques like lowercasing, removing punctuation, and eliminating stopwords (common words without significant meaning). We also use stemming or lemmatization to normalize words and reduce them to their base form. Additionally, tokenization is applied to break the text into smaller units like sentences or words.\n",
        "\n",
        "3. Now that the data is cleaned up, we can proceed with clustering. However, like folding clothes to save space in the closet, we first need to reduce the dimensionality of the data. This step helps in efficient clustering. Various algorithms can be employed to cluster the movies, and we can use techniques to determine the optimal number of clusters.\n",
        "\n",
        "4. After constructing the optimal clusters, we can explore their contents using word clouds, which are like showcasing the unique personality of each cluster. Word clouds visually represent the most frequently occurring words in each cluster in a creative and engaging manner. Through this, we gain insights into the distinctive characteristics that define each cluster and identify the common patterns that connect the movies within each group."
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Selecting Attributes**"
      ],
      "metadata": {
        "id": "4uCs_TbaD5N_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a \"tags\" Column from Textual Data for Model Building\n",
        "df['tags'] = df['description'] + df['listed_in'] + df['rating'] + df['cast'] + df['country'] + df['director']"
      ],
      "metadata": {
        "id": "ZGnn_qq3Dvk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the \"tags\" column data for the first row (index 0) of the DataFrame 'df'.\n",
        "tags_data = df.tags[0]\n",
        "\n",
        "# Displaying the \"tags\" data for the first row.\n",
        "print(tags_data)\n"
      ],
      "metadata": {
        "id": "ncSGkID4Dwkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The consolidation of all essential data into a single column has been completed successfully.\n",
        "\n",
        "Removing Stopwords and Lower Casing: In natural language processing (NLP) tasks, standard pre-processing steps often involve the removal of stop words and converting words to lowercase.\n",
        "\n",
        "Stop words are frequently used words in a language that lack substantial meaning, like \"a,\" \"an,\" \"the,\" and \"is.\" Since they can introduce unnecessary noise and potentially impact the performance of NLP models, it is common to exclude them as a pre-processing measure.\n",
        "\n",
        "Lowercasing words involves converting all the words in a text to lowercase. This is a prevalent pre-processing technique in NLP tasks for several beneficial reasons:\n",
        "\n",
        "1. Case differences can be ignored: By lowercasing the words, you can treat words with different capitalization as the same word, which can be useful in tasks such as information retrieval or text classification where case differences are not important.\n",
        "\n",
        "2. Vocabulary size is reduced: Lowercasing the words can also reduce the size of the vocabulary, which can make it easier to work with larger texts or texts in languages with a high number of inflected forms."
      ],
      "metadata": {
        "id": "N0aNZYtcEJtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Removing Stopwords**"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "# Step 1: Check if the stop words list is already downloaded; if not, download it.\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Step 2: Create a set of English stop words.\n",
        "#          Stop words are common words in the English language that are often removed from text data for NLP tasks.\n",
        "#          By creating a set of stop words, we can efficiently check for their presence in text and exclude them if necessary.\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Step 3: Display the set of stop words.\n",
        "#          By converting the list of stop words into a NumPy array, we can easily examine and work with the stop words.\n",
        "import numpy as np\n",
        "stop_words_array = np.array(list(stop_words))\n",
        "print(stop_words_array)"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the stopwords\n",
        "def stopwords(text):\n",
        "    '''a function for removing the stopword and lowercase the each word'''\n",
        "    text = [word.lower() for word in text.split() if word.lower() not in stop_words]\n",
        "    # Joining the list of words with space separator\n",
        "    return \" \".join(text)"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the stopwords function\n",
        "df['tags'] = df['tags'].apply(stopwords)"
      ],
      "metadata": {
        "id": "y_gi86qxE7uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Displaying the \"tags\" data for the first row.\n",
        "df.tags[0]"
      ],
      "metadata": {
        "id": "rXGMRNn-E83_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process of removing all the stopwords and converting the corpus to lowercase has been completed successfully."
      ],
      "metadata": {
        "id": "_cl3w7e1HA2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Removing Puntuation**"
      ],
      "metadata": {
        "id": "RdO_-6nZGbFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In NLP tasks, it is customary to remove punctuation as a standard pre-processing step. Punctuation marks, such as periods, commas, and exclamation points, can introduce unnecessary noise to the data and may be treated as separate tokens, potentially impacting the performance of NLP models."
      ],
      "metadata": {
        "id": "51wfPgDGHHX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove punctuations\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    '''a function for removing punctuation'''\n",
        "    import string\n",
        "    # Replacing the punctuations with no space, which in effect deletes the punctuation marks.\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # Return the text stripped of punctuation marks\n",
        "    return text.translate(translator)"
      ],
      "metadata": {
        "id": "t1V3sDLMGgRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the remove_punctuation function\n",
        "df['tags'] = df['tags'].apply(remove_punctuation)"
      ],
      "metadata": {
        "id": "FgBmcoG5Gx1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the \"tags\" data for the first row.\n",
        "df.tags[0]"
      ],
      "metadata": {
        "id": "kroF56oAGzPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The successful removal of all punctuation marks from the corpus has been achieved."
      ],
      "metadata": {
        "id": "Lw8FMompFzKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stemming**"
      ],
      "metadata": {
        "id": "Zu4N2YJkGFLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SnowballStemmer was utilized to transform a corpus of words into meaningful word stems. Stemming is a prevalent pre-processing technique in NLP tasks, which involves reducing words to their base or root form. This process enables treating various inflected forms of a word as equivalent, proving advantageous for tasks such as information retrieval or text classification.\n",
        "\n",
        "For instance, words like \"run,\" \"runs,\" \"ran,\" and \"running\" can all be stemmed to their base form \"run,\" facilitating better analysis and understanding of the text data."
      ],
      "metadata": {
        "id": "wQKFubwlHN0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import the SnowballStemmer from NLTK library.\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# Step 2: Create an instance of the SnowballStemmer with the language specified as \"english\".\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Step 3: Define the 'stemming' function.\n",
        "#         This function takes a text as input and stems each word in the text using the SnowballStemmer.\n",
        "def stemming(text):\n",
        "    # Step 3.1: Split the input text into individual words.\n",
        "    words = text.split()\n",
        "\n",
        "    # Step 3.2: Apply stemming to each word in the list using the stemmer object.\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # Step 3.3: Join the stemmed words back into a single string.\n",
        "    stemmed_text = \" \".join(stemmed_words)\n",
        "\n",
        "    # Step 3.4: Return the resulting stemmed text.\n",
        "    return stemmed_text"
      ],
      "metadata": {
        "id": "rUwWVzS6HWNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the stemming function.\n",
        "df['tags'] = df['tags'].apply(stemming)"
      ],
      "metadata": {
        "id": "jQi31QBQHZpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the \"tags\" data for the first row.\n",
        "df.tags[0]"
      ],
      "metadata": {
        "id": "jahO_iDSGI8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The corpus has undergone stemming process."
      ],
      "metadata": {
        "id": "0JOW2zuSHkHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Vectorization**"
      ],
      "metadata": {
        "id": "yLutV6E4H0dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word or text vectorization is the process of converting words into numerical vectors. This is crucial in NLP tasks because many machine learning models require numerical input and cannot directly handle raw text data. Word vectorization enables representing words in a manner that retains their meaning and context, making it possible to use them as input for machine learning models. Additionally, word vectorization can be utilized to gauge the similarity between words through vector arithmetic."
      ],
      "metadata": {
        "id": "gqYoJvLqH97u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the object of tfid vectorizer\n",
        "tfidf = TfidfVectorizer(stop_words='english', lowercase=False, max_features = 10000)   # Max features = 10000 to prevent system from crashing\n",
        "\n",
        "# Fitting the vectorizer using the text data\n",
        "tfidf.fit(df['tags'])\n",
        "\n",
        "# Collecting the vocabulary items used in the vectorizer\n",
        "dictionary = tfidf.vocabulary_.items()\n"
      ],
      "metadata": {
        "id": "YH8oGeNnH88M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the results\n",
        "print(len(dictionary)) # Number of independent features created from \"tags\" columns > max_features = 10000"
      ],
      "metadata": {
        "id": "KVb7KLQaIE64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the vector into array form for clustering\n",
        "vector = tfidf.transform(df['tags']).toarray()\n",
        "\n",
        "# Summarizing the encoded vector\n",
        "print(vector)\n",
        "print(f'Shape of the vector : {vector.shape}')\n",
        "print(f'Datatype : {type(vector)}')"
      ],
      "metadata": {
        "id": "uCgPvzbfIL9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Dimesionality Reduction**\n",
        "\n"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA (Principal Component Analysis) can be employed to reduce the dimensionality of data.\n",
        "\n",
        "Dimensionality reduction refers to the process of decreasing the number of features or dimensions in a dataset while retaining relevant information. This step is frequently employed in machine learning and data analysis due to the challenges posed by high-dimensional datasets and the potential issues associated with the curse of dimensionality."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import the necessary library for PCA.\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Step 2: Create an instance of the PCA class with a specified random state for reproducibility.\n",
        "pca = PCA(random_state=42)\n",
        "\n",
        "# Step 3: Fit the PCA model to the input data (vector).\n",
        "#         PCA will learn the principal components from the data and reduce the dimensionality.\n",
        "pca.fit(vector)"
      ],
      "metadata": {
        "id": "uf2eneSaKcfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explained variance for different number of components\n",
        "\n",
        "# Step 1: Create a figure with a specific size to plot the results.\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Step 2: Plot the cumulative explained variance for different numbers of components.\n",
        "#         The cumulative explained variance is obtained from the PCA object (pca) using 'explained_variance_ratio_'.\n",
        "#         np.cumsum() is used to compute the cumulative sum of explained variance for each component.\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "\n",
        "# Step 3: Customize the plot with title, axis labels, and style.\n",
        "plt.title('PCA - Cumulative Explained Variance vs Number of Components')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "\n",
        "# Step 4: Add horizontal and vertical lines to indicate certain thresholds or points of interest.\n",
        "#         In this case, we add a red dashed line at y=0.8 to show the 80% cumulative explained variance threshold.\n",
        "#         We also add a green dashed line at x=3000 to mark the number of components at a specific point of interest.\n",
        "plt.axhline(y=0.8, color='red', linestyle='--')\n",
        "plt.axvline(x=3000, color='green', linestyle='--')\n",
        "\n",
        "# Step 5: Display the plot.\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8RWh4e5WpOiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. After analyzing the cumulative explained variance plot, we observe that approximately ~7500 components are required to explain 100% of the variance in the data.\n",
        "\n",
        "2. However, a significant portion of the variance (more than 80%) can be explained by just 3000 components.\n",
        "\n",
        "3. Therefore, to simplify the model and reduce dimensionality while still capturing a substantial amount of variance, we can choose to retain the top 3000 components. This decision will enable us to maintain a balance between model complexity and explanatory power, as it still accounts for more than 80% of the total variance in the data."
      ],
      "metadata": {
        "id": "0cbyuMrbLQe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducing the dimensions to 3000 using PCA\n",
        "\n",
        "# Step 1: Create an instance of the PCA class with the desired number of components (n_components=3000).\n",
        "#         Set a random state for reproducibility of results.\n",
        "pca = PCA(n_components=3000, random_state=42)\n",
        "\n",
        "# Step 2: Fit the PCA model to the input data (vector) with the chosen number of components (3000).\n",
        "#         PCA will learn the principal components from the data and reduce the dimensionality to 3000 components.\n",
        "pca.fit(vector)"
      ],
      "metadata": {
        "id": "RRKBk0T-Lde4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Transform the input data 'vector' using the previously fitted PCA model.\n",
        "#         The transformed data will have reduced dimensions based on the chosen number of components (3000).\n",
        "X = pca.transform(vector)\n",
        "\n",
        "# Step 2: Check the shape of the transformed data 'X'.\n",
        "#         The 'shape' attribute of the transformed data provides the number of rows (samples) and columns (components).\n",
        "transformed_shape = X.shape\n",
        "\n",
        "# Step 3: Print the shape of the transformed data to view the number of samples and components.\n",
        "print(transformed_shape)\n"
      ],
      "metadata": {
        "id": "-X_K5MBLMRMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1\n",
        "# K-Means Clustering"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-means clustering is an unsupervised machine learning method employed to partition a dataset into a predetermined number of clusters. It is referred to as \"unsupervised\" because the algorithm does not depend on labeled examples for learning. Instead, it leverages the intrinsic patterns within the data to group the samples into clusters.\n",
        "\n",
        "How it Works?\n",
        "\n",
        "1. The k-means algorithm begins by randomly choosing k initial \"centroids\" or cluster centers from the data.\n",
        "\n",
        "2. Next, it assigns each sample in the dataset to the closest centroid, using a distance metric like Euclidean distance.\n",
        "\n",
        "3. The algorithm then updates the centroids by calculating their means based on the samples in each cluster.\n",
        "\n",
        "4. It iteratively repeats the process of reassigning samples to the nearest centroids and updating the centroids until convergence.\n",
        "\n",
        "5. To determine the optimal number of clusters for the K-means algorithm, one can visualize the elbow curve and Silhouette score. These methods help in finding the number of clusters that best capture the data's underlying structure."
      ],
      "metadata": {
        "id": "A_IrHwkcUaxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Elbow Method\n",
        "'''Elbow method to find the optimal value of k'''\n",
        "\n",
        "# Initialize an empty list to store the sum of squared errors for each value of k\n",
        "sum_squared_errors = []\n",
        "\n",
        "# Loop through different values of k from 1 to 15\n",
        "for k in range(1, 16):\n",
        "    # Initialize the KMeans model with the current value of k\n",
        "    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
        "\n",
        "    # Fit the KMeans model to the data\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    # Compute the sum of squared errors for the current model\n",
        "    sum_squared_errors.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the values of sum of squared errors against the number of clusters (k)\n",
        "plt.plot(range(1, 16), sum_squared_errors)\n",
        "plt.title('The Elbow Method - KMeans clustering')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Sum of squared errors')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Silhoutte score method\n",
        "'''Silhouette score method to find the optimal value of k'''\n",
        "\n",
        "# Initialize an empty list to store the silhouette score for each value of k (number of clusters)\n",
        "silhouette_avg = []\n",
        "# Loop through different values of k from 2 to 15\n",
        "for k in range(2, 16):\n",
        "    # Initialize the KMeans model with the current value of k\n",
        "    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
        "\n",
        "    # Fit the KMeans model to the data (X)\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    # Predict the cluster labels for each data point\n",
        "    labels = kmeans.labels_\n",
        "\n",
        "    # Compute the silhouette score for the current model\n",
        "    score = silhouette_score(X, labels)\n",
        "\n",
        "    # Append the computed silhouette score to the silhouette_avg list\n",
        "    silhouette_avg.append(score)\n",
        "\n",
        "# Plot the Silhouette analysis\n",
        "plt.plot(range(2, 16), silhouette_avg)\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette score')\n",
        "plt.title('Silhouette Analysis for Optimal k - KMeans Clustering')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q5Oj6BIee5Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering the data into 5 clusters\n",
        "\n",
        "# Initialize the KMeans model with 5 clusters, using 'k-means++' as the initialization method and setting random_state to 33 for reproducibility\n",
        "kmeans = KMeans(n_clusters=5, init='k-means++', random_state=33)\n",
        "\n",
        "# Fit the KMeans model to the data (X)\n",
        "kmeans.fit(X)"
      ],
      "metadata": {
        "id": "G8KmFC-nfIkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the distortion (inertia) and Silhouette score for the KMeans clustering result\n",
        "kmeans_distortion = kmeans.inertia_\n",
        "kmeans_silhouette_score = silhouette_score(X, kmeans.labels_)\n",
        "\n",
        "# Print the calculated distortion and Silhouette score\n",
        "print(\"Distortion (Inertia):\", kmeans_distortion)\n",
        "print(\"Silhouette Score:\", kmeans_silhouette_score)\n"
      ],
      "metadata": {
        "id": "pp3EBvsZfK-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column 'kmeans_cluster' to the DataFrame 'df' containing the cluster numbers assigned by KMeans\n",
        "df['kmeans_cluster'] = kmeans.labels_\n"
      ],
      "metadata": {
        "id": "eNhvRco5fOxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a random sample of 5 rows from the DataFrame 'df', showing selected columns along with the KMeans cluster number\n",
        "df.sample(5)[['type', 'title', 'director', 'cast', 'country', 'rating', 'listed_in', 'description', 'kmeans_cluster']]"
      ],
      "metadata": {
        "id": "nt1c5QmRfPnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size for the plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "# Create a countplot to show the number of movies and TV shows in each KMeans cluster, with 'type' as the hue\n",
        "graph = sns.countplot(x='kmeans_cluster', data=df, hue='type')\n",
        "\n",
        "# Set the title for the plot\n",
        "plt.title('Number of Movies and TV Shows in Each Cluster - KMeans Clustering')\n",
        "\n",
        "# Add value counts on top of each bar to display the actual counts\n",
        "for p in graph.patches:\n",
        "    # Get the height of the bar\n",
        "    bar_height = p.get_height()\n",
        "    # Annotate the bar with its value count, rounded to integer using '.0f' format\n",
        "    graph.annotate(format(bar_height, '.0f'), (p.get_x() + p.get_width() / 2, bar_height),\n",
        "                   ha='center', va='bottom')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yRUYkhfcfXhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The k-means clustering algorithm has been applied to successfully create 5 distinct clusters."
      ],
      "metadata": {
        "id": "J3YzMXwkfbF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building wordclouds for different clusters in K-Means Clustering**"
      ],
      "metadata": {
        "id": "n_L_2aLMfcDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for building a word cloud for movies/shows in a specific cluster\n",
        "def kmeans_wordcloud(cluster_number, column_name):\n",
        "    '''\n",
        "    Build a word cloud for the specified cluster.\n",
        "\n",
        "    Parameters:\n",
        "        cluster_number (int): The cluster number for which the word cloud will be created.\n",
        "        column_name (str): The name of the column containing text data (e.g., 'description', 'cast').\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    '''\n",
        "\n",
        "    # Create a DataFrame containing the specified cluster's data for the given column and remove any NaN values\n",
        "    df_wordcloud = df[['kmeans_cluster', column_name]].dropna()\n",
        "    df_wordcloud = df_wordcloud[df_wordcloud['kmeans_cluster'] == cluster_number]\n",
        "\n",
        "    # Combine all text documents into a single string\n",
        "    text = \" \".join(word for word in df_wordcloud[column_name])\n",
        "\n",
        "    # Create the word cloud using the WordCloud class from the wordcloud library\n",
        "    wordcloud = WordCloud(stopwords=set(STOPWORDS), background_color=\"white\").generate(text)\n",
        "\n",
        "    # Display the word cloud image\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "N-d60wEGflh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Cloud on \"description\" column for different cluster**"
      ],
      "metadata": {
        "id": "o931Pj0tfqVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Description"
      ],
      "metadata": {
        "id": "V41dJUnaf1lH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# description\n",
        "for i in range(5):\n",
        "  print(f'cluster {i}')\n",
        "  kmeans_wordcloud(i,'description')"
      ],
      "metadata": {
        "id": "nZISzFZWfz3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Cloud on \"cast\" column for different cluster**"
      ],
      "metadata": {
        "id": "YghCN94Vm8bt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cast"
      ],
      "metadata": {
        "id": "eQuhJaPWm91W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cast\n",
        "for i in range(5):\n",
        "  print(f'cluster {i}')\n",
        "  kmeans_wordcloud(i,'cast')"
      ],
      "metadata": {
        "id": "gI-7H2ixnCpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Cloud on \"director\" column for different cluster**"
      ],
      "metadata": {
        "id": "vfj5IGInnWmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Director"
      ],
      "metadata": {
        "id": "i2zAzm9YnhuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# director\n",
        "for i in range(5):\n",
        "  print(f'cluster {i}')\n",
        "  kmeans_wordcloud(i,'director')"
      ],
      "metadata": {
        "id": "xWP7fS7xnnT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Cloud on \"country\" column for different cluster**"
      ],
      "metadata": {
        "id": "LSZpfh5kr_uH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Country"
      ],
      "metadata": {
        "id": "kTVRjevAsIXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# country\n",
        "for i in range(5):\n",
        "  print(f'cluster {i}')\n",
        "  kmeans_wordcloud(i,'country')"
      ],
      "metadata": {
        "id": "uUNSJX-2sOZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Cloud on \"title\" column for different cluster**"
      ],
      "metadata": {
        "id": "sAKB1yhKsYPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Title"
      ],
      "metadata": {
        "id": "1_wxQWUKsixi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# title\n",
        "for i in range(5):\n",
        "  print(f'cluster {i}')\n",
        "  kmeans_wordcloud(i,'title')"
      ],
      "metadata": {
        "id": "EKLVSR5jsmbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2\n",
        "# **Hirearchical Clustering**\n",
        "After applying the agglomerative hierarchical clustering algorithm, the resulting clusters are visualized in a dendrogram, which represents a tree-like structure. The dendrogram provides insights into the relationships between the clusters at each level of the hierarchy.\n",
        "\n",
        "To identify the ideal number of clusters for our dataset, we examine the dendrogram visually. We look for the most extended vertical distance that does not intersect any horizontal line. This distance indicates the greatest dissimilarity between any two merged clusters.\n",
        "\n",
        "By drawing a horizontal line at this distance and counting the number of vertical lines it intersects, we can determine the optimal number of clusters for our data. This number represents the most suitable segmentation of the dataset into distinct groups."
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a dendogram to decide the number of clusters\n",
        "\n",
        "# Set the figure size for the dendrogram plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Create the dendrogram using the linkage method 'ward' and plot it\n",
        "dendrogram = shc.dendrogram(shc.linkage(X, method='ward'))\n",
        "\n",
        "# Set the title, x-axis label, and y-axis label for the dendrogram plot\n",
        "plt.title('Dendrogram for Netflix Shows - Hierarchical Clustering')\n",
        "plt.xlabel('Netflix Shows')\n",
        "plt.ylabel('Distance')\n",
        "\n",
        "# Draw a horizontal line at y=4 to mark the threshold distance for identifying clusters\n",
        "plt.axhline(y=4, color='r', linestyle='--')\n",
        "\n",
        "# Display the dendrogram plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5q9iEFoC0ATt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**At a distance of 4 units, 7 clusters can be built using the agglomerative clustering algorithm.**"
      ],
      "metadata": {
        "id": "Z6FuvZgX0LTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building 7 clusters using the Agglomerative clustering algorithm**"
      ],
      "metadata": {
        "id": "NfaLXyh_0Oid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Hierarchical Clustering model with 7 clusters, using Euclidean distance as the affinity measure and Ward linkage for clustering\n",
        "hierarchical = AgglomerativeClustering(n_clusters=7, affinity='euclidean', linkage='ward')\n",
        "\n",
        "# Fit the model to the data and obtain the cluster labels for each data point\n",
        "cluster_labels = hierarchical.fit_predict(X)"
      ],
      "metadata": {
        "id": "Fc1ZtvVn0ZtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column 'hierarchical_cluster' in the DataFrame 'df'\n",
        "# The values for this column are obtained from the hierarchical clustering model's labels attribute\n",
        "df['hierarchical_cluster'] = hierarchical.labels_"
      ],
      "metadata": {
        "id": "Jf7Gzacj0amX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a random sample of 5 rows from the DataFrame 'df', showing specific columns along with the hierarchical cluster number\n",
        "df.sample(5)[['type', 'title', 'director', 'cast', 'country', 'rating', 'listed_in', 'description', 'hierarchical_cluster']]\n"
      ],
      "metadata": {
        "id": "lFaRiLsO0_uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of movies and tv shows in each cluster\n",
        "\n",
        "# Set the figure size for the plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Create a countplot to show the number of movies and TV shows in each hierarchical cluster, with 'type' as the hue\n",
        "graph = sns.countplot(x='hierarchical_cluster', data=df, hue='type')\n",
        "\n",
        "# Set the title for the plot\n",
        "plt.title('Number of Movies and TV Shows in Each Cluster - Hierarchical Clustering')\n",
        "\n",
        "# Add value counts on top of each bar to display the actual counts\n",
        "for p in graph.patches:\n",
        "    # Get the height of the bar\n",
        "    bar_height = p.get_height()\n",
        "    # Annotate the bar with its value count, rounded to integer using '.0f' format\n",
        "    graph.annotate(format(bar_height, '.0f'), (p.get_x() + p.get_width() / 2, bar_height),\n",
        "                   ha='center', va='bottom')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y9hMCRw71RXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Agglomerative (hierarchical) clustering algorithm has been applied successfully to create 7 distinct clusters.**"
      ],
      "metadata": {
        "id": "iOSCDwzZ2x5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building wordclouds for different clusters in Hierarchical Clustering**"
      ],
      "metadata": {
        "id": "tWDzzf5V2zmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for building a word cloud for movies/shows in a specific hierarchical cluster\n",
        "def hierarchical_wordcloud(cluster_number, column_name):\n",
        "    '''\n",
        "    Build a word cloud for the specified hierarchical cluster.\n",
        "\n",
        "    Parameters:\n",
        "        cluster_number (int): The cluster number for which the word cloud will be created.\n",
        "        column_name (str): The name of the column containing text data (e.g., 'description', 'cast').\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    '''\n",
        "\n",
        "    # Create a DataFrame containing the specified hierarchical cluster's data for the given column and remove any NaN values\n",
        "    df_wordcloud = df[['hierarchical_cluster', column_name]].dropna()\n",
        "    df_wordcloud = df_wordcloud[df_wordcloud['hierarchical_cluster'] == cluster_number]\n",
        "\n",
        "    # Combine all text documents into a single string\n",
        "    text = \" \".join(word for word in df_wordcloud[column_name])\n",
        "\n",
        "    # Create the word cloud using the WordCloud class from the wordcloud library\n",
        "    wordcloud = WordCloud(stopwords=set(STOPWORDS), background_color=\"white\").generate(text)\n",
        "\n",
        "    # Display the word cloud image\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "JLEvpkBT3HOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Cloud on \"title\" column for different cluster**"
      ],
      "metadata": {
        "id": "z3l2E7JS3Ygy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Title"
      ],
      "metadata": {
        "id": "MJoS23EO3-5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(7):\n",
        "  print(f'cluster {i}')\n",
        "  hierarchical_wordcloud(i,'title')"
      ],
      "metadata": {
        "id": "IrUApGWr1e85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Cloud on \"description\" column for different cluster**"
      ],
      "metadata": {
        "id": "Nt4QZ-5_9LY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Description"
      ],
      "metadata": {
        "id": "kAywCgjXBPZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# description\n",
        "for i in range(7):\n",
        "  print(f'cluster {i}')\n",
        "  hierarchical_wordcloud(i,'description')"
      ],
      "metadata": {
        "id": "IOWGjIiPBf9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Cloud on \"cast\" column for different cluster**"
      ],
      "metadata": {
        "id": "yEb8ik4SB2ib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cast"
      ],
      "metadata": {
        "id": "3wjnhsyeB4zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cast\n",
        "for i in range(7):\n",
        "  print(f'cluster {i}')\n",
        "  hierarchical_wordcloud(i,'cast')"
      ],
      "metadata": {
        "id": "6oFZYaopCNCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Cloud on \"Country\" column for different cluster**"
      ],
      "metadata": {
        "id": "C8GdLG20CyH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Country"
      ],
      "metadata": {
        "id": "LD5aJWczCTV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(7):\n",
        "  print(f'cluster {i}')\n",
        "  hierarchical_wordcloud(i,'country')"
      ],
      "metadata": {
        "id": "BkfzVQYJCUIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Cloud on \"listed_in (genre)\" column for different cluster**"
      ],
      "metadata": {
        "id": "5DTOPpj3Cr_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listed in"
      ],
      "metadata": {
        "id": "tfCrbziHCje5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(7):\n",
        "  print(f'cluster {i}')\n",
        "  hierarchical_wordcloud(i,'listed_in')"
      ],
      "metadata": {
        "id": "eiOTpursCh7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Content Based Recommendation System**"
      ],
      "metadata": {
        "id": "D274OY4udVwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Content-based recommendation systems suggest items to users based on the similarity of those items. These systems make recommendations by analyzing the description or features of products. The goal is to find items that are similar to the ones the user has shown interest in.\n",
        "\n",
        "2. The content-based recommender system keeps track of the items that a specific user likes. It also considers the preferences and interests of users who share similar behavior and likings. This information is used to recommend items to the user based on their preferences and those of users with similar tastes.\n",
        "\n",
        "3. A simple content-based recommender system can be built by measuring the similarity between movies or shows on Netflix. The system uses techniques like cosine similarity to assess the similarity between the descriptions or features of different shows.\n",
        "\n",
        "4. Cosine similarity is a measure used to calculate the similarity score between two vectors, such as the descriptions of shows. It is obtained by taking the dot product of the two vectors and dividing it by the magnitude of the vectors. A higher cosine similarity score indicates a smaller angle between the vectors and suggests greater similarity between the corresponding shows."
      ],
      "metadata": {
        "id": "B99gxpYVdUi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting specific columns from the DataFrame 'df'\n",
        "\n",
        "# Specify the columns to be displayed in the output\n",
        "selected_columns = ['show_id', 'title', 'tags']\n",
        "\n",
        "# Create a new DataFrame by selecting the specified columns from 'df'\n",
        "selected_data = df[selected_columns]\n",
        "\n",
        "# Display the new DataFrame, showing the 'show_id', 'title', and 'tags' columns\n",
        "selected_data\n"
      ],
      "metadata": {
        "id": "drvY15-zdffP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The DataFrame contains a total of 7770 rows. However, some rows with null values were removed, and the last index displayed in the DataFrame is 7786.\n",
        "\n",
        "2. To construct a content-based recommendation system, we compute the similarity score based on a specific index_id relative to the corresponding \"tags\" column.\n",
        "\n",
        "3. Neglecting to reset the index could lead to miscalculating the cosine similarity for a different index, resulting in inaccurate recommendations. Therefore, resetting the index is vital to ensure that the recommendations are based on the correct index.\n",
        "\n",
        "4. Resetting the index involves assigning a new sequential index to each row in the DataFrame, starting from 0. This ensures that each row has a unique and identifiable index, facilitating computations and yielding precise outcomes.\n",
        "\n",
        "5. Consequently, resetting the index plays a pivotal role in developing a content-based recommendation system, guaranteeing that the recommendations are aligned with the correct index, and thus, leading to more accurate and relevant suggestions."
      ],
      "metadata": {
        "id": "aa-y9vj7dkJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new DataFrame 'recommender_df' for building the recommendation system\n",
        "\n",
        "# Make a copy of the original DataFrame 'df' to create a new DataFrame 'recommender_df'\n",
        "recommender_df = df.copy()\n",
        "\n",
        "# Reset the index of 'recommender_df' to ensure accurate index alignment for the recommendation system\n",
        "recommender_df.reset_index(inplace=True)\n",
        "\n",
        "# Display specific columns 'show_id', 'title', and 'tags' of the DataFrame 'recommender_df' to verify the index reset\n",
        "recommender_df[['show_id', 'title', 'tags']]"
      ],
      "metadata": {
        "id": "wrMZJpcGdo0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing 'show_id' and 'index' columns from the DataFrame 'recommender_df'\n",
        "\n",
        "# Drop the specified columns 'show_id' and 'index' from the DataFrame 'recommender_df'\n",
        "recommender_df.drop(columns=['index', 'show_id'], inplace=True)"
      ],
      "metadata": {
        "id": "Sp_QEFD4dsTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the index for the DataFrame 'recommender_df'\n",
        "\n",
        "# Before resetting the index, find the index position of the movie titled 'Zero' in the original DataFrame 'df'\n",
        "print(f\"Before resetting index, the index ID for the movie 'Zero' is: {df[df['title'] == 'Zero'].index[0]}\")\n",
        "\n",
        "# After resetting the index, find the index position of the movie titled 'Zero' in the modified DataFrame 'recommender_df'\n",
        "print(f\"After resetting index, the index ID for the movie 'Zero' is: {recommender_df[recommender_df['title'] == 'Zero'].index[0]}\")"
      ],
      "metadata": {
        "id": "VLJayAgndxFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing the transformed array of independent features created from the 'tags' column after performing PCA for dimensionality reduction\n",
        "X"
      ],
      "metadata": {
        "id": "iqLOnPtKd-Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cosine similarity for the transformed feature array 'X'\n",
        "similarity = cosine_similarity(X)\n",
        "similarity"
      ],
      "metadata": {
        "id": "H_TEoHc1gF_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to list down top 10 recommended movies on the basis of cosine similarity score**"
      ],
      "metadata": {
        "id": "Y5xsufc0gMg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to recommend top ten movies based on similarity score for a given movie\n",
        "\n",
        "def recommend(movie):\n",
        "    '''\n",
        "    This function recommends the top ten movies similar to the given movie based on similarity scores.\n",
        "\n",
        "    Parameters:\n",
        "        movie (str): The title of the movie for which recommendations are to be made.\n",
        "\n",
        "    Returns:\n",
        "        None (Prints the recommended movie titles)\n",
        "    '''\n",
        "\n",
        "    # Display the message indicating the input movie\n",
        "    print(f\"If you liked '{movie}', you may also enjoy: \\n\")\n",
        "\n",
        "    # Find the index position of the input movie in the DataFrame 'recommender_df'\n",
        "    index = recommender_df[recommender_df['title'] == movie].index[0]\n",
        "\n",
        "    # Sort the similarity scores in descending order, to identify movies with the highest similarity to the input movie\n",
        "    distances = sorted(list(enumerate(similarity[index])), reverse=True, key=lambda x: x[1])\n",
        "\n",
        "    # Display the top ten recommended movies, excluding the input movie itself\n",
        "    for i in distances[1:11]:\n",
        "        print(df.iloc[i[0]].title)"
      ],
      "metadata": {
        "id": "6_J05OeCgHPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommendation for the movie Naruto\n",
        "recommend('Naruto')"
      ],
      "metadata": {
        "id": "5BJplC0lgPTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommendation for the movie Phir Hera Pheri\n",
        "recommend('Phir Hera Pheri')"
      ],
      "metadata": {
        "id": "phTzNB8igQAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thank You! for being a part of this thrilling journey of exploring the captivating realm of Netflix shows! Our aim was to group the shows into clusters based on their similarities and differences, ultimately crafting a content-based recommender system that suggests 10 shows based on users' viewing history.**\n",
        "\n",
        "1. With a dataset comprising more than 7787 records and 11 attributes, we embarked on our adventure by examining missing values and conducting exploratory data analysis (EDA). Our discoveries revealed that Netflix boasts a larger collection of movies than TV shows, with a burgeoning array of shows hailing from the United States.\n",
        "\n",
        "2. To cluster the shows effectively, we focused on six crucial attributes: director, cast, country, genre, rating, and description. These attributes were transformed into a 10000-feature TFIDF vectorization, and Principal Component Analysis (PCA) was employed to address the challenges of high dimensionality. By reducing the components to 3000, we successfully captured over 80% of the variance.\n",
        "\n",
        "3. Next, we employed two clustering algorithms, K-Means and Agglomerative clustering, to group the shows. K-Means indicated that 5 clusters were optimal, a finding confirmed by the elbow method and Silhouette score analysis. Meanwhile, Agglomerative clustering suggested 7 clusters, which we visualized using a dendrogram.\n",
        "\n",
        "4. However, our journey didn't end there. We took it a step further by creating a content-based recommender system using the similarity matrix obtained through cosine similarity. This remarkable system provides personalized recommendations based on users' previous show selections, presenting them with 10 exceptional suggestions to explore.\n",
        "\n",
        "**I invite you to join me in unraveling the diverse world of Netflix shows, as my recommender system guides you towards your next captivating binge-worthy fascination.**"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}